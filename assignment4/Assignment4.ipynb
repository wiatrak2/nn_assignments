{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4\n",
    "\n",
    "**Submission deadline: last lab session before or on Wednesday, 22.11.17**\n",
    "\n",
    "**Points: 11 + 4 bonus points**\n",
    "\n",
    "\n",
    "## Downloading this notebook\n",
    "\n",
    "This assignment is an Jupyter notebook. Download it by cloning https://github.com/janchorowski/nn_assignments. Follow the instructions in its README for instructions.\n",
    "\n",
    "Please do not hesitate to use GitHubâ€™s pull requests to send us corrections!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starter code: network for Irises in Pytorch\n",
    "\n",
    "\n",
    "In the following cells a feedforward neural network has been implemented with the aid of PyTorch and its autograd mechanism. Please study the code - many network implementations follow a similar pattern.\n",
    "\n",
    "The provided network trains to nearly 100% accuracy on Iris using Batch Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class Layer(object):\n",
    "    \n",
    "    def train_mode(self):\n",
    "        self.train_m = True\n",
    "    \n",
    "    def eval_mode(self):\n",
    "        self.train_m = False\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "\n",
    "class AffineLayer(Layer):\n",
    "    def __init__(self, num_in, num_out):\n",
    "        self.W = Variable(torch.FloatTensor(num_in, num_out),\n",
    "                          requires_grad=True)\n",
    "        self.W.name = 'W'\n",
    "        self.b = Variable(torch.FloatTensor(1, num_out),\n",
    "                          requires_grad=True)\n",
    "        self.b.name = 'b'\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return [self.W, self.b]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x.mm(self.W) + self.b\n",
    "\n",
    "    \n",
    "class TanhLayer(Layer):\n",
    "    def forward(self, x):\n",
    "        return F.tanh(x)\n",
    "\n",
    "    \n",
    "class  ReLULayer(Layer):\n",
    "    def forward(self, x):\n",
    "        return F.relu(x)\n",
    "\n",
    "\n",
    "class SoftMaxLayer(Layer):\n",
    "    def forward(self, x):\n",
    "        return F.softmax(x)\n",
    "    \n",
    "class DropoutLayer(Layer):\n",
    "    def __init__(self, p=0.5):\n",
    "        self.pr = p\n",
    "    def forward(self, x):\n",
    "        return F.dropout(x, self.pr, self.train_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeedforwardNet(object):\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params += layer.parameters\n",
    "        return params\n",
    "\n",
    "    @parameters.setter\n",
    "    def parameters(self, values):\n",
    "        for ownP, newP in zip(self.parameters, values):\n",
    "            ownP.data = newP.data\n",
    "    \n",
    "    def train_mode(self):\n",
    "        for layer in self.layers:\n",
    "            layer.train_mode()\n",
    "    \n",
    "    def eval_mode(self):\n",
    "        for layer in self.layers:\n",
    "            layer.eval_mode()    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, outputs, targets):\n",
    "        return torch.mean(-torch.log(torch.gather(\n",
    "            outputs, 1, targets.unsqueeze(1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import torchvision\n",
    "\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "IrisX = iris.data.astype(np.float32)\n",
    "IrisX = (IrisX - IrisX.mean(axis=0, keepdims=True)) / IrisX.std(axis=0, keepdims=True)\n",
    "IrisY = iris.target\n",
    "\n",
    "def GD(model, x, y, alpha=1e-4, max_iters=1000000, tolerance=1e-6):\n",
    "    \"\"\"Simple batch gradient descent\"\"\"\n",
    "    try:\n",
    "        old_loss = np.inf\n",
    "        x = Variable(torch.from_numpy(x), requires_grad=False)\n",
    "        y = Variable(torch.from_numpy(y.astype(np.int64)), requires_grad=False)\n",
    "        model.train_mode()\n",
    "        for i in xrange(max_iters):\n",
    "            outputs = model.forward(x)\n",
    "            loss = model.loss(outputs, y)\n",
    "\n",
    "            loss.backward()\n",
    "            for p in model.parameters:\n",
    "                p.data -= p.grad.data * alpha\n",
    "                # Zero gradients for the next iteration\n",
    "                p.grad.data.zero_()\n",
    "\n",
    "            loss = loss.data[0]\n",
    "            if old_loss < loss:\n",
    "                print \"Iter: %d, loss increased!\" % (i,)\n",
    "            if (old_loss - loss) < tolerance:\n",
    "                print \"Tolerance level reached. Exiting.\"\n",
    "                break\n",
    "            if i % 1000 == 0:\n",
    "                _, predictions = outputs.data.max(dim=1)\n",
    "                err_rate = 100.0 * (predictions != y.data).sum() / outputs.size(0)\n",
    "                print \"Iteration {0: >6} | loss {1: >5.2f} | err rate  {2: >5.2f}%\" \\\n",
    "                      .format(i, loss, err_rate)\n",
    "            old_loss = loss\n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration      0 | loss  1.12 | err rate  94.00%\n",
      "Iteration   1000 | loss  0.06 | err rate   2.00%\n",
      "Iteration   2000 | loss  0.04 | err rate   2.00%\n",
      "Iteration   3000 | loss  0.04 | err rate   1.33%\n",
      "Iteration   4000 | loss  0.04 | err rate   1.33%\n",
      "Iteration   5000 | loss  0.04 | err rate   1.33%\n",
      "Iteration   6000 | loss  0.04 | err rate   1.33%\n",
      "Iteration   7000 | loss  0.04 | err rate   1.33%\n",
      "Iteration   8000 | loss  0.04 | err rate   1.33%\n",
      "Iteration   9000 | loss  0.04 | err rate   1.33%\n",
      "Iteration  10000 | loss  0.04 | err rate   1.33%\n",
      "Iteration  11000 | loss  0.04 | err rate   1.33%\n",
      "Iteration  12000 | loss  0.04 | err rate   1.33%\n",
      "Iteration  13000 | loss  0.04 | err rate   1.33%\n",
      "Iteration  14000 | loss  0.04 | err rate   1.33%\n",
      "Iteration  15000 | loss  0.04 | err rate   1.33%\n",
      "Iteration  16000 | loss  0.04 | err rate   1.33%\n",
      "Iteration  17000 | loss  0.03 | err rate   1.33%\n",
      "Iteration  18000 | loss  0.03 | err rate   1.33%\n",
      "Iteration  19000 | loss  0.03 | err rate   1.33%\n",
      "Iteration  20000 | loss  0.03 | err rate   1.33%\n",
      "Iteration  21000 | loss  0.03 | err rate   1.33%\n",
      "Iteration  22000 | loss  0.03 | err rate   1.33%\n",
      "Iteration  23000 | loss  0.03 | err rate   1.33%\n",
      "Iteration  24000 | loss  0.03 | err rate   1.33%\n",
      "Iteration  25000 | loss  0.03 | err rate   1.33%\n",
      "Iteration  26000 | loss  0.03 | err rate   1.33%\n",
      "Iteration  27000 | loss  0.03 | err rate   1.33%\n",
      "Iteration  28000 | loss  0.03 | err rate   1.33%\n",
      "Iteration  29000 | loss  0.03 | err rate   1.33%\n",
      "Iteration  30000 | loss  0.03 | err rate   1.33%\n",
      "Iteration  31000 | loss  0.03 | err rate   1.33%\n",
      "Iteration  32000 | loss  0.03 | err rate   1.33%\n",
      "Iteration  33000 | loss  0.03 | err rate   1.33%\n",
      "Iteration  34000 | loss  0.03 | err rate   1.33%\n",
      "Iteration  35000 | loss  0.03 | err rate   1.33%\n",
      "Iteration  36000 | loss  0.03 | err rate   1.33%\n",
      "Iteration  37000 | loss  0.03 | err rate   1.33%\n",
      "Iteration  38000 | loss  0.03 | err rate   1.33%\n",
      "Iteration  39000 | loss  0.03 | err rate   1.33%\n",
      "Iteration  40000 | loss  0.02 | err rate   0.67%\n",
      "Iteration  41000 | loss  0.02 | err rate   0.67%\n",
      "Iteration  42000 | loss  0.02 | err rate   0.67%\n",
      "Iteration  43000 | loss  0.02 | err rate   0.67%\n",
      "Iteration  44000 | loss  0.02 | err rate   0.67%\n",
      "Iteration  45000 | loss  0.01 | err rate   0.67%\n",
      "Iteration  46000 | loss  0.01 | err rate   0.67%\n",
      "Iteration  47000 | loss  0.01 | err rate   0.00%\n",
      "Iteration  48000 | loss  0.01 | err rate   0.00%\n",
      "Iteration  49000 | loss  0.01 | err rate   0.00%\n",
      "Iteration  50000 | loss  0.01 | err rate   0.00%\n",
      "Iteration  51000 | loss  0.01 | err rate   0.00%\n",
      "Iteration  52000 | loss  0.01 | err rate   0.00%\n",
      "Iteration  53000 | loss  0.01 | err rate   0.00%\n",
      "Iteration  54000 | loss  0.00 | err rate   0.00%\n",
      "Iteration  55000 | loss  0.00 | err rate   0.00%\n",
      "Iteration  56000 | loss  0.00 | err rate   0.00%\n",
      "Iteration  57000 | loss  0.00 | err rate   0.00%\n",
      "Iteration  58000 | loss  0.00 | err rate   0.00%\n",
      "Iteration  59000 | loss  0.00 | err rate   0.00%\n",
      "Iteration  60000 | loss  0.00 | err rate   0.00%\n",
      "Iteration  61000 | loss  0.00 | err rate   0.00%\n",
      "Iteration  62000 | loss  0.00 | err rate   0.00%\n",
      "Iteration  63000 | loss  0.00 | err rate   0.00%\n",
      "Iteration  64000 | loss  0.00 | err rate   0.00%\n",
      "Iteration  65000 | loss  0.00 | err rate   0.00%\n",
      "Tolerance level reached. Exiting.\n"
     ]
    }
   ],
   "source": [
    "model = FeedforwardNet(\n",
    "    [AffineLayer(4, 10),\n",
    "     TanhLayer(),\n",
    "     AffineLayer(10, 3),\n",
    "     SoftMaxLayer(),\n",
    "    ])\n",
    "\n",
    "# Initialize parameters\n",
    "for p in model.parameters:\n",
    "    if p.name == 'W':\n",
    "        # p.data.normal_(0, 0.05)\n",
    "        p.data.uniform_(-0.1, 0.1)\n",
    "    elif p.name == 'b':\n",
    "        p.data.zero_()\n",
    "    else:\n",
    "        raise ValueError('Unknown parameter name \"%s\"' % p.name)\n",
    "\n",
    "# Train\n",
    "GD(model, IrisX, IrisY, alpha=1e-1, tolerance=1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starter code for MNIST and SGD scaffolding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import PIL\n",
    "from PIL import ImageFilter  \n",
    "\n",
    "batch_size = 128\n",
    "data_path = os.environ.get('PYTORCH_DATA_PATH', '../data')\n",
    "\n",
    "transform = torchvision.transforms.Compose(\n",
    "    [\n",
    "     torchvision.transforms.ToTensor(),\n",
    "     torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    ])\n",
    "\n",
    "def rot(img):\n",
    "    img = img.rotate(np.random.rand() * 7)\n",
    "    if np.random.rand() < 0.8:\n",
    "        ar = np.array(img)\n",
    "        gauss = np.random.normal(0,0.1,ar.shape)\n",
    "        gauss = gauss.reshape(ar.shape[0],-1)\n",
    "        ar = ar + gauss\n",
    "        img = PIL.Image.fromarray(numpy.uint8(ar))\n",
    "    if np.random.rand() < 0.7:\n",
    "        img = img.filter(ImageFilter.GaussianBlur(0.2))\n",
    "    return img\n",
    "\n",
    "transform_train = torchvision.transforms.Compose(\n",
    "    [\n",
    "     torchvision.transforms.Lambda(rot),\n",
    "     torchvision.transforms.ToTensor(),\n",
    "     torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    ])\n",
    "\n",
    "_test = torchvision.datasets.MNIST(\n",
    "    data_path, train=False, download=True, transform=transform)\n",
    "\n",
    "# Load training data, split into train and valid sets\n",
    "_train = torchvision.datasets.MNIST(\n",
    "    data_path, train=True, download=True, transform=transform)\n",
    "_train.train_data = _train.train_data[:50000]\n",
    "_train.train_labels = _train.train_labels[:50000]\n",
    "\n",
    "_train2 = torchvision.datasets.MNIST(\n",
    "    data_path, train=True, download=True, transform=transform_train)\n",
    "_train2.train_data = _train2.train_data[:50000]\n",
    "_train2.train_labels = _train2.train_labels[:50000]\n",
    "\n",
    "_train = torch.utils.data.dataset.ConcatDataset([_train, _train2])\n",
    "\n",
    "_valid = torchvision.datasets.MNIST(\n",
    "    data_path, train=True, download=True, transform=transform)\n",
    "_valid.train_data = _valid.train_data[50000:]\n",
    "_valid.train_labels = _valid.train_labels[50000:]\n",
    "\n",
    "mnist_loaders = {\n",
    "    'train': torch.utils.data.DataLoader(\n",
    "        _train, batch_size=batch_size, shuffle=True),\n",
    "    'valid': torch.utils.data.DataLoader(\n",
    "        _valid, batch_size=batch_size, shuffle=False),\n",
    "    'test': torch.utils.data.DataLoader(\n",
    "        _test, batch_size=batch_size, shuffle=False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4ef61b6210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mnist = torchvision.datasets.MNIST(\n",
    "    data_path, train=True, download=True, transform=transform_train)    \n",
    "image = mnist[13]\n",
    "\n",
    "im = image[0][0].numpy()\n",
    "imshow(im, cmap='gray')\n",
    "\n",
    "print len(_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stats.stackexchange.com/questions/29130/difference-between-neural-net-weight-decay-and-learning-rate\n",
    "http://ruder.io/optimizing-gradient-descent/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def compute_error_rate(model, data_loader, cuda=False):\n",
    "    model.eval_mode()\n",
    "    num_errs = 0.0\n",
    "    num_examples = 0\n",
    "    for x, y in data_loader:\n",
    "        if cuda:\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "        x = Variable(x.view(x.size(0), -1), volatile=True)\n",
    "        y = Variable(y, volatile=True)\n",
    "        outputs = model.forward(x)\n",
    "        _, predictions = outputs.data.max(dim=1)\n",
    "        num_errs += (predictions != y.data).sum()\n",
    "        num_examples += x.size(0)\n",
    "    return 100.0 * num_errs / num_examples\n",
    "\n",
    "def SGD(model, data_loaders, alpha=1e-4, num_epochs=1, patience_expansion=1.5,\n",
    "        log_every=100, cuda=False):\n",
    "    print model.parameters\n",
    "    if cuda:\n",
    "        for p in model.parameters:\n",
    "            p.data = p.data.cuda()\n",
    "    #\n",
    "    # TODO: Initialize momentum variables\n",
    "    # Hint: You need one velocity matrix for each parameter\n",
    "    #\n",
    "    # velocities = \n",
    "    #\n",
    "    velocities = [Variable(torch.zeros(p.size()), volatile=True) for p in model.parameters]\n",
    "    if cuda:\n",
    "        velocities = [v.cuda() for v in velocities]\n",
    "    \n",
    "    decay_rate = 1e-4\n",
    "    alpha0 = alpha\n",
    "    alpha_rate = 0.9995\n",
    "    \n",
    "    iter_ = 0\n",
    "    epoch = 0\n",
    "    best_params = None\n",
    "    best_val_err = np.inf\n",
    "    history = {'train_losses': [], 'train_errs': [], 'val_errs': []}\n",
    "    print('Training the model!')\n",
    "    print('Interrupt at any time to evaluate the best validation model so far.')\n",
    "    try:\n",
    "        while epoch < num_epochs:\n",
    "            model.train_mode()\n",
    "            epoch += 1\n",
    "            for x, y in data_loaders['train']:\n",
    "                if cuda:\n",
    "                    x = x.cuda()\n",
    "                    y = y.cuda()\n",
    "                iter_ += 1\n",
    "                x = Variable(x.view(x.size(0), -1), requires_grad=False)\n",
    "                y = Variable(y, requires_grad=False)\n",
    "                \n",
    "                out = model.forward(x)\n",
    "                loss = model.loss(out, y)\n",
    "                loss.backward()\n",
    "                _, predictions = out.data.max(dim=1)\n",
    "                err_rate = 100.0 * (predictions != y.data).sum() / out.size(0)\n",
    "\n",
    "                history['train_losses'].append(loss.data[0])\n",
    "                history['train_errs'].append(err_rate)\n",
    "\n",
    "                for p, v in zip(model.parameters, velocities):\n",
    "                    if p.name == 'W':\n",
    "                        p.grad.data += decay_rate*p.data\n",
    "                    alpha = alpha0 * alpha_rate ** iter_\n",
    "                    epsilon = 0.8\n",
    "                    v.data = epsilon*v.data + alpha*p.grad.data \n",
    "                    p.data = p.data - v.data\n",
    "                    #\n",
    "                    # TODO: Implement weight decay addition to gradients\n",
    "                    # TODO: Update learning rate\n",
    "                    # Hint: Use the iteration counter i\n",
    "                    # alpha = TODO\n",
    "                    # TODO: Set the momentum constant \n",
    "                    # TODO: Implement velocity update in momentum\n",
    "                    # v[...] = TODO\n",
    "                    # TODO: Set a more sensible learning rule here,\n",
    "                    #       using your learning rate schedule and momentum\n",
    "                    # \n",
    "                    \n",
    "                    # Zero gradients for the next iteration\n",
    "                    p.grad.data.zero_()\n",
    "\n",
    "                if iter_ % log_every == 0:\n",
    "                    print \"Minibatch {0: >6}  | loss {1: >5.2f} | err rate {2: >5.2f}%\" \\\n",
    "                          .format(iter_, loss.data[0], err_rate)\n",
    "            \n",
    "\n",
    "            val_err_rate = compute_error_rate(model, data_loaders['valid'], cuda)\n",
    "            history['val_errs'].append((iter_, val_err_rate))\n",
    "            \n",
    "            if val_err_rate < best_val_err:\n",
    "                # Adjust num of epochs\n",
    "                num_epochs = int(np.maximum(num_epochs, epoch * patience_expansion + 1))\n",
    "                best_epoch = epoch\n",
    "                best_val_err = val_err_rate\n",
    "                best_params = [p.clone().cpu() for p in model.parameters]\n",
    "            m = \"After epoch {0: >2} | valid err rate: {1: >5.2f}% | doing {2: >3} epochs\" \\\n",
    "                .format(epoch, val_err_rate, num_epochs)\n",
    "            print '{0}\\n{1}\\n{0}'.format('-' * len(m), m)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    if best_params is not None:\n",
    "        print \"\\nLoading best params on validation set (epoch %d)\\n\" %(best_epoch)\n",
    "        model.parameters = best_params\n",
    "    plot_history(history)\n",
    "\n",
    "def plot_history(history):\n",
    "    figsize(16, 4)\n",
    "    plt.subplot(1,2,1)\n",
    "    train_loss = np.array(history['train_losses'])\n",
    "    semilogy(np.arange(train_loss.shape[0]), train_loss, label='batch train loss')\n",
    "    legend()\n",
    "        \n",
    "    plt.subplot(1,2,2)\n",
    "    train_errs = np.array(history['train_errs'])\n",
    "    plt.plot(np.arange(train_errs.shape[0]), train_errs, label='batch train error rate')\n",
    "    val_errs = np.array(history['val_errs'])\n",
    "    plt.plot(val_errs[:,0], val_errs[:,1], label='validation error rate', color='r')\n",
    "    ylim(0,20)\n",
    "    legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Stochastic Gradient Descent [3p]\n",
    "Implement the following additions to the SGD code provided above:\n",
    "  1. **[1p]** momentum\n",
    "  2. **[1p]** learning rate schedule\n",
    "  3. **[1p]** weight decay, in which we additionally minimize for each weight matrix (but typically not the bias) the sum of its elements squared. One way to implement it is to use function `model.parameters` and select all parameters whose names are \"`W`\" and not \"`b`\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Tuning the Network for MNIST [4p]\n",
    "\n",
    "Tune the following network to reach **validation error rate below 1.9%**.\n",
    "This should result in a **test error rate below 2%**. To\n",
    "tune the network you will need to:\n",
    "1. Choose the number of layers (more than 1, less than 5);\n",
    "2. Choose the number of neurons in each layer (more than 100,\n",
    "    less than 5000);\n",
    "3. Pick proper weight initialization;\n",
    "4. Pick proper learning rate schedule (need to decay over time,\n",
    "    good range to check on MNIST is about 1e-2 ... 1e-1 at the beginning and\n",
    "    half of that after 10000 batches);\n",
    "5. Pick a momentum constant (probably a constant one will be OK).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Variable containing:\n",
      "-1.6939e-02  1.5748e-03 -9.3383e-03  ...  -3.4262e-03  1.5337e-02 -9.0300e-03\n",
      " 4.6661e-03 -1.8960e-02  6.5268e-03  ...   7.4571e-03  2.1039e-03 -7.0380e-03\n",
      "-1.4003e-03 -7.7122e-03  1.3217e-02  ...   1.5647e-03  1.2828e-02 -9.0752e-03\n",
      "                ...                   â‹±                   ...                \n",
      " 6.8404e-03  8.0044e-03  8.9478e-03  ...   1.4701e-02  7.7452e-04  5.7638e-03\n",
      "-7.0831e-03  9.8543e-03  2.4146e-03  ...  -1.2783e-02  7.1351e-03 -8.4689e-03\n",
      " 1.6668e-03  2.7409e-03 -6.4624e-03  ...   3.7608e-03  6.4053e-03 -1.6378e-03\n",
      "[torch.FloatTensor of size 784x1600]\n",
      ", Variable containing:\n",
      "    0     0     0  ...      0     0     0\n",
      "[torch.FloatTensor of size 1x1600]\n",
      ", Variable containing:\n",
      " 1.9385e-02  1.7924e-02  4.8489e-03  ...  -5.2424e-03 -3.7529e-03 -3.1102e-03\n",
      "-1.1311e-02 -7.7776e-03 -1.2398e-02  ...  -2.1836e-02 -1.3059e-03  6.1977e-03\n",
      "-9.7941e-03  2.7856e-03  2.8653e-03  ...   4.9489e-03  1.9088e-03 -4.1067e-03\n",
      "                ...                   â‹±                   ...                \n",
      "-4.0269e-03 -3.7635e-04 -6.6517e-03  ...   2.2709e-03 -1.5917e-02  1.4030e-03\n",
      " 1.6805e-02 -4.5598e-03  1.1840e-02  ...  -4.3249e-03  1.2953e-02  1.2083e-02\n",
      " 2.4482e-03  5.5616e-03 -1.6502e-02  ...   8.8357e-03 -5.0823e-03  4.4647e-03\n",
      "[torch.FloatTensor of size 1600x800]\n",
      ", Variable containing:\n",
      "\n",
      "Columns 0 to 12 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 13 to 25 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 26 to 38 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 39 to 51 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 52 to 64 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 65 to 77 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 78 to 90 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 91 to 103 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 104 to 116 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 117 to 129 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 130 to 142 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 143 to 155 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 156 to 168 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 169 to 181 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 182 to 194 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 195 to 207 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 208 to 220 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 221 to 233 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 234 to 246 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 247 to 259 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 260 to 272 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 273 to 285 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 286 to 298 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 299 to 311 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 312 to 324 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 325 to 337 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 338 to 350 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 351 to 363 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 364 to 376 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 377 to 389 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 390 to 402 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 403 to 415 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 416 to 428 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 429 to 441 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 442 to 454 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 455 to 467 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 468 to 480 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 481 to 493 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 494 to 506 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 507 to 519 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 520 to 532 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 533 to 545 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 546 to 558 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 559 to 571 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 572 to 584 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 585 to 597 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 598 to 610 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 611 to 623 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 624 to 636 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 637 to 649 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 650 to 662 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 663 to 675 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 676 to 688 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 689 to 701 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 702 to 714 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 715 to 727 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 728 to 740 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 741 to 753 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 754 to 766 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 767 to 779 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 780 to 792 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 793 to 799 \n",
      "    0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 1x800]\n",
      ", Variable containing:\n",
      "-9.4357e-03 -9.5863e-03  1.2193e-02  ...   1.8473e-02  5.6946e-03 -1.1311e-02\n",
      " 2.0404e-02 -1.0265e-02 -1.1438e-02  ...  -2.8544e-03  1.0468e-02 -1.9263e-03\n",
      " 2.9003e-03 -2.5570e-02  1.5079e-02  ...  -4.5119e-03  2.1804e-02 -2.5184e-02\n",
      "                ...                   â‹±                   ...                \n",
      " 7.9908e-03  1.4590e-03  1.2725e-02  ...  -9.1699e-03 -1.3826e-02 -1.9400e-02\n",
      "-1.7883e-02 -4.8612e-03 -5.7198e-04  ...  -1.0516e-02 -3.5246e-04 -2.1306e-02\n",
      " 4.2779e-03 -7.1639e-03 -1.1408e-02  ...  -1.0967e-02  9.2356e-03  3.5443e-03\n",
      "[torch.FloatTensor of size 800x400]\n",
      ", Variable containing:\n",
      "\n",
      "Columns 0 to 12 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 13 to 25 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 26 to 38 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 39 to 51 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 52 to 64 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 65 to 77 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 78 to 90 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 91 to 103 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 104 to 116 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 117 to 129 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 130 to 142 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 143 to 155 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 156 to 168 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 169 to 181 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 182 to 194 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 195 to 207 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 208 to 220 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 221 to 233 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 234 to 246 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 247 to 259 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 260 to 272 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 273 to 285 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 286 to 298 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 299 to 311 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 312 to 324 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 325 to 337 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 338 to 350 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 351 to 363 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 364 to 376 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 377 to 389 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 390 to 399 \n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 1x400]\n",
      ", Variable containing:\n",
      " 9.2736e-03 -1.0348e-02 -1.2263e-02  ...   1.1220e-02 -3.1719e-04  1.2852e-02\n",
      " 3.5792e-03 -1.2520e-02 -3.0672e-03  ...   4.7128e-03 -7.2273e-04 -1.7163e-02\n",
      "-5.6619e-03 -6.6332e-03 -1.8587e-02  ...  -4.1092e-03 -2.8935e-03  8.6129e-03\n",
      "                ...                   â‹±                   ...                \n",
      " 4.2932e-03  5.4131e-03 -1.8048e-02  ...  -1.7070e-02 -8.5525e-03 -3.0662e-03\n",
      "-1.1454e-02 -1.5680e-02 -7.7326e-03  ...  -6.3495e-03 -6.6377e-03  5.4288e-04\n",
      "-2.3402e-02 -9.3114e-03  8.0050e-03  ...  -9.6768e-03 -7.4228e-03 -9.4572e-03\n",
      "[torch.FloatTensor of size 400x10]\n",
      ", Variable containing:\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 1x10]\n",
      "]\n",
      "Training the model!\n",
      "Interrupt at any time to evaluate the best validation model so far.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch    100  | loss  0.36 | err rate 10.94%\n",
      "Minibatch    200  | loss  0.22 | err rate  6.25%\n",
      "Minibatch    300  | loss  0.13 | err rate  4.69%\n",
      "Minibatch    400  | loss  0.10 | err rate  3.12%\n",
      "Minibatch    500  | loss  0.14 | err rate  4.69%\n",
      "Minibatch    600  | loss  0.05 | err rate  1.56%\n",
      "Minibatch    700  | loss  0.06 | err rate  1.56%\n",
      "----------------------------------------------------------\n",
      "After epoch  1 | valid err rate:  2.43% | doing   2 epochs\n",
      "----------------------------------------------------------\n",
      "Minibatch    800  | loss  0.03 | err rate  0.78%\n",
      "Minibatch    900  | loss  0.01 | err rate  0.00%\n",
      "Minibatch   1000  | loss  0.08 | err rate  3.91%\n",
      "Minibatch   1100  | loss  0.02 | err rate  0.00%\n",
      "Minibatch   1200  | loss  0.08 | err rate  2.34%\n",
      "Minibatch   1300  | loss  0.06 | err rate  2.34%\n",
      "Minibatch   1400  | loss  0.04 | err rate  0.78%\n",
      "Minibatch   1500  | loss  0.06 | err rate  1.56%\n",
      "----------------------------------------------------------\n",
      "After epoch  2 | valid err rate:  1.85% | doing   4 epochs\n",
      "----------------------------------------------------------\n",
      "Minibatch   1600  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   1700  | loss  0.01 | err rate  0.00%\n",
      "Minibatch   1800  | loss  0.01 | err rate  0.00%\n",
      "Minibatch   1900  | loss  0.03 | err rate  0.78%\n",
      "Minibatch   2000  | loss  0.05 | err rate  1.56%\n",
      "Minibatch   2100  | loss  0.02 | err rate  1.56%\n",
      "Minibatch   2200  | loss  0.01 | err rate  0.00%\n",
      "Minibatch   2300  | loss  0.00 | err rate  0.00%\n",
      "----------------------------------------------------------\n",
      "After epoch  3 | valid err rate:  1.80% | doing   5 epochs\n",
      "----------------------------------------------------------\n",
      "Minibatch   2400  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   2500  | loss  0.02 | err rate  0.78%\n",
      "Minibatch   2600  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   2700  | loss  0.01 | err rate  0.00%\n",
      "Minibatch   2800  | loss  0.04 | err rate  1.56%\n",
      "Minibatch   2900  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   3000  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   3100  | loss  0.02 | err rate  0.78%\n",
      "----------------------------------------------------------\n",
      "After epoch  4 | valid err rate:  1.64% | doing   7 epochs\n",
      "----------------------------------------------------------\n",
      "Minibatch   3200  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   3300  | loss  0.01 | err rate  0.00%\n",
      "Minibatch   3400  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   3500  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   3600  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   3700  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   3800  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   3900  | loss  0.01 | err rate  0.00%\n",
      "----------------------------------------------------------\n",
      "After epoch  5 | valid err rate:  1.51% | doing   8 epochs\n",
      "----------------------------------------------------------\n",
      "Minibatch   4000  | loss  0.01 | err rate  0.78%\n",
      "Minibatch   4100  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   4200  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   4300  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   4400  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   4500  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   4600  | loss  0.00 | err rate  0.00%\n",
      "----------------------------------------------------------\n",
      "After epoch  6 | valid err rate:  1.54% | doing   8 epochs\n",
      "----------------------------------------------------------\n",
      "Minibatch   4700  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   4800  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   4900  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   5000  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   5100  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   5200  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   5300  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   5400  | loss  0.00 | err rate  0.00%\n",
      "----------------------------------------------------------\n",
      "After epoch  7 | valid err rate:  1.54% | doing   8 epochs\n",
      "----------------------------------------------------------\n",
      "Minibatch   5500  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   5600  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   5700  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   5800  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   5900  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   6000  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   6100  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   6200  | loss  0.00 | err rate  0.00%\n",
      "----------------------------------------------------------\n",
      "After epoch  8 | valid err rate:  1.48% | doing  13 epochs\n",
      "----------------------------------------------------------\n",
      "Minibatch   6300  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   6400  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   6500  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   6600  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   6700  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   6800  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   6900  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   7000  | loss  0.00 | err rate  0.00%\n",
      "----------------------------------------------------------\n",
      "After epoch  9 | valid err rate:  1.53% | doing  13 epochs\n",
      "----------------------------------------------------------\n",
      "Minibatch   7100  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   7200  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   7300  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   7400  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   7500  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   7600  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   7700  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   7800  | loss  0.00 | err rate  0.00%\n",
      "----------------------------------------------------------\n",
      "After epoch 10 | valid err rate:  1.51% | doing  13 epochs\n",
      "----------------------------------------------------------\n",
      "Minibatch   7900  | loss  0.01 | err rate  0.78%\n",
      "Minibatch   8000  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   8100  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   8200  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   8300  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   8400  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   8500  | loss  0.01 | err rate  0.78%\n",
      "Minibatch   8600  | loss  0.00 | err rate  0.00%\n",
      "----------------------------------------------------------\n",
      "After epoch 11 | valid err rate:  1.56% | doing  13 epochs\n",
      "----------------------------------------------------------\n",
      "Minibatch   8700  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   8800  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   8900  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   9000  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   9100  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   9200  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   9300  | loss  0.00 | err rate  0.00%\n",
      "----------------------------------------------------------\n",
      "After epoch 12 | valid err rate:  1.53% | doing  13 epochs\n",
      "----------------------------------------------------------\n",
      "Minibatch   9400  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   9500  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   9600  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   9700  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   9800  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   9900  | loss  0.00 | err rate  0.00%\n",
      "Minibatch  10000  | loss  0.00 | err rate  0.00%\n",
      "Minibatch  10100  | loss  0.00 | err rate  0.00%\n",
      "----------------------------------------------------------\n",
      "After epoch 13 | valid err rate:  1.54% | doing  13 epochs\n",
      "----------------------------------------------------------\n",
      "\n",
      "Loading best params on validation set (epoch 8)\n",
      "\n",
      "Test error rate: 1.42%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA64AAAD8CAYAAAB3qPkTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVSmllFJKKaVUTNPEVSmllFJKKaVUTPv/mi435nHbihIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4f49a30550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# TODO: Pick a network architecture here.\n",
    "#       The one below is just softmax regression.\n",
    "#\n",
    "\n",
    "model = FeedforwardNet(\n",
    "    [  \n",
    "     AffineLayer(784, 1600),\n",
    "     ReLULayer(),\n",
    "     AffineLayer(1600, 800),\n",
    "     ReLULayer(),\n",
    "     AffineLayer(800, 400),\n",
    "     ReLULayer(),\n",
    "     AffineLayer(400, 10),\n",
    "     SoftMaxLayer()\n",
    "    ])\n",
    "\n",
    "# Initialize parameters\n",
    "for p in model.parameters:\n",
    "    if p.name == 'W':\n",
    "        p.data.normal_(0, 0.01)\n",
    "        # p.data.uniform_(-0.1, 0.1)\n",
    "    elif p.name == 'b':\n",
    "        p.data.zero_()\n",
    "    else:\n",
    "        raise ValueError('Unknown parameter name \"%s\"' % p.name)\n",
    "\n",
    "# On lab computers you can set cuda=True !\n",
    "SGD(model, mnist_loaders, alpha=1e-1, cuda=True)\n",
    "\n",
    "print \"Test error rate: %.2f%%\" % compute_error_rate(model, mnist_loaders['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Dropout [2p]\n",
    "\n",
    "Implement a **dropout** layer and try to train a\n",
    "network getting below 1.5% test error rates with dropout. The best\n",
    "results with dropout are below 1%!\n",
    "\n",
    "Remember to turn off dropout during testing, using `model.train_mode()` and `model.eval_mode()`!\n",
    "\n",
    "Hint: Use [torch.nn.functional.dropout](http://pytorch.org/docs/master/nn.html#torch.nn.functional.dropout).\n",
    "\n",
    "Details: http://arxiv.org/pdf/1207.0580.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Variable containing:\n",
      " 2.7374e-03 -2.4131e-03  2.6287e-03  ...  -1.3014e-02  4.3618e-03  3.5185e-03\n",
      " 9.9782e-03  2.1757e-02  1.5890e-02  ...   8.0913e-04 -7.9763e-03 -3.3038e-03\n",
      "-1.0752e-02  7.0584e-03 -6.2892e-03  ...   2.6859e-03  1.4167e-03 -7.6575e-03\n",
      "                ...                   â‹±                   ...                \n",
      " 1.0121e-02  1.3406e-02  5.8925e-03  ...  -1.4098e-02 -7.4911e-03  2.0252e-02\n",
      "-9.1766e-03 -9.6979e-03  7.5332e-03  ...  -8.0189e-03  2.3202e-02 -7.5670e-03\n",
      " 9.3263e-03 -2.1846e-03 -5.5927e-03  ...  -1.4925e-02  1.4906e-02 -1.2070e-04\n",
      "[torch.FloatTensor of size 784x1600]\n",
      ", Variable containing:\n",
      "    0     0     0  ...      0     0     0\n",
      "[torch.FloatTensor of size 1x1600]\n",
      ", Variable containing:\n",
      " 4.1255e-03 -2.0104e-02 -4.3663e-03  ...   1.4165e-02  1.2168e-02 -8.4960e-03\n",
      "-1.6323e-02  3.8388e-04  5.1990e-04  ...  -6.4982e-03  1.9515e-03  2.0250e-02\n",
      "-2.2156e-03 -2.1806e-02  1.9435e-02  ...   3.3278e-03  4.8433e-03 -9.4168e-03\n",
      "                ...                   â‹±                   ...                \n",
      " 6.9628e-03  1.6474e-02  4.7687e-03  ...   2.7787e-03 -4.6265e-03 -6.1032e-03\n",
      " 3.9039e-03 -5.2885e-03  9.0814e-03  ...   1.2700e-02  8.4470e-03 -1.9378e-02\n",
      " 1.8926e-03  2.4231e-03  3.4237e-03  ...  -1.3804e-02 -8.6340e-03 -6.4937e-03\n",
      "[torch.FloatTensor of size 1600x800]\n",
      ", Variable containing:\n",
      "\n",
      "Columns 0 to 12 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 13 to 25 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 26 to 38 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 39 to 51 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 52 to 64 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 65 to 77 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 78 to 90 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 91 to 103 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 104 to 116 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 117 to 129 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 130 to 142 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 143 to 155 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 156 to 168 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 169 to 181 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 182 to 194 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 195 to 207 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 208 to 220 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 221 to 233 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 234 to 246 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 247 to 259 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 260 to 272 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 273 to 285 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 286 to 298 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 299 to 311 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 312 to 324 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 325 to 337 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 338 to 350 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 351 to 363 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 364 to 376 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 377 to 389 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 390 to 402 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 403 to 415 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 416 to 428 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 429 to 441 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 442 to 454 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 455 to 467 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 468 to 480 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 481 to 493 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 494 to 506 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 507 to 519 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 520 to 532 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 533 to 545 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 546 to 558 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 559 to 571 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 572 to 584 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 585 to 597 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 598 to 610 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 611 to 623 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 624 to 636 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 637 to 649 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 650 to 662 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 663 to 675 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 676 to 688 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 689 to 701 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 702 to 714 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 715 to 727 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 728 to 740 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 741 to 753 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 754 to 766 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 767 to 779 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 780 to 792 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 793 to 799 \n",
      "    0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 1x800]\n",
      ", Variable containing:\n",
      "-1.2707e-02  8.7473e-03  5.6396e-03  ...   1.5928e-03 -2.4876e-02 -1.8182e-03\n",
      "-1.6157e-03  5.3285e-03 -1.1545e-02  ...  -3.4753e-03  6.6528e-03  3.3238e-03\n",
      " 6.4942e-03  8.9382e-03  6.3949e-03  ...  -2.1880e-02 -1.0460e-03  5.4773e-03\n",
      "                ...                   â‹±                   ...                \n",
      "-3.4469e-03  6.6845e-03  6.6633e-03  ...  -1.2939e-02  6.8692e-04 -1.1358e-02\n",
      "-1.3258e-03  8.6682e-03 -1.5001e-03  ...  -1.2325e-02  2.4140e-04  3.5844e-03\n",
      "-1.2471e-02  3.1057e-04  8.3939e-04  ...  -8.3552e-03 -1.4855e-02 -1.6124e-04\n",
      "[torch.FloatTensor of size 800x400]\n",
      ", Variable containing:\n",
      "\n",
      "Columns 0 to 12 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 13 to 25 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 26 to 38 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 39 to 51 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 52 to 64 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 65 to 77 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 78 to 90 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 91 to 103 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 104 to 116 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 117 to 129 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 130 to 142 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 143 to 155 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 156 to 168 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 169 to 181 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 182 to 194 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 195 to 207 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 208 to 220 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 221 to 233 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 234 to 246 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 247 to 259 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 260 to 272 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 273 to 285 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 286 to 298 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 299 to 311 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 312 to 324 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 325 to 337 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 338 to 350 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 351 to 363 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 364 to 376 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 377 to 389 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 390 to 399 \n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 1x400]\n",
      ", Variable containing:\n",
      "-5.8705e-03 -2.6671e-03  4.1013e-03  ...   4.7688e-03 -5.2573e-03 -4.8264e-03\n",
      " 2.3321e-02 -6.4978e-03  1.0859e-02  ...  -1.9353e-03  5.6469e-03  3.4942e-03\n",
      "-5.8504e-03  9.1839e-03  1.4760e-03  ...  -4.0482e-03 -9.9787e-04  9.5934e-03\n",
      "                ...                   â‹±                   ...                \n",
      "-6.8432e-03  1.0490e-02 -7.3501e-03  ...   6.5115e-03  4.0415e-03 -4.9915e-03\n",
      "-4.3471e-04 -1.4339e-03 -1.0941e-02  ...  -5.5334e-03 -1.2440e-02  5.9577e-04\n",
      " 1.3181e-02  3.0518e-03  3.5144e-03  ...   2.2167e-02  2.1026e-02  2.3137e-03\n",
      "[torch.FloatTensor of size 400x10]\n",
      ", Variable containing:\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 1x10]\n",
      "]\n",
      "Training the model!\n",
      "Interrupt at any time to evaluate the best validation model so far.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch    100  | loss  0.61 | err rate 21.88%\n",
      "Minibatch    200  | loss  0.38 | err rate 10.16%\n",
      "Minibatch    300  | loss  0.27 | err rate  7.81%\n",
      "Minibatch    400  | loss  0.37 | err rate 12.50%\n",
      "Minibatch    500  | loss  0.13 | err rate  3.91%\n",
      "Minibatch    600  | loss  0.13 | err rate  2.34%\n",
      "Minibatch    700  | loss  0.22 | err rate  4.69%\n",
      "----------------------------------------------------------\n",
      "After epoch  1 | valid err rate:  3.06% | doing   2 epochs\n",
      "----------------------------------------------------------\n",
      "Minibatch    800  | loss  0.14 | err rate  3.91%\n",
      "Minibatch    900  | loss  0.20 | err rate  8.59%\n",
      "Minibatch   1000  | loss  0.13 | err rate  3.91%\n",
      "Minibatch   1100  | loss  0.20 | err rate  3.91%\n",
      "Minibatch   1200  | loss  0.19 | err rate  5.47%\n",
      "Minibatch   1300  | loss  0.05 | err rate  0.78%\n",
      "Minibatch   1400  | loss  0.16 | err rate  3.12%\n",
      "Minibatch   1500  | loss  0.10 | err rate  4.69%\n",
      "----------------------------------------------------------\n",
      "After epoch  2 | valid err rate:  2.24% | doing   4 epochs\n",
      "----------------------------------------------------------\n",
      "Minibatch   1600  | loss  0.14 | err rate  3.12%\n",
      "Minibatch   1700  | loss  0.11 | err rate  3.12%\n",
      "Minibatch   1800  | loss  0.12 | err rate  3.12%\n",
      "Minibatch   1900  | loss  0.09 | err rate  1.56%\n",
      "Minibatch   2000  | loss  0.08 | err rate  3.12%\n",
      "Minibatch   2100  | loss  0.12 | err rate  3.91%\n",
      "Minibatch   2200  | loss  0.05 | err rate  1.56%\n",
      "Minibatch   2300  | loss  0.11 | err rate  2.34%\n",
      "----------------------------------------------------------\n",
      "After epoch  3 | valid err rate:  1.97% | doing   5 epochs\n",
      "----------------------------------------------------------\n",
      "Minibatch   2400  | loss  0.11 | err rate  4.69%\n",
      "Minibatch   2500  | loss  0.05 | err rate  1.56%\n",
      "Minibatch   2600  | loss  0.05 | err rate  2.34%\n",
      "Minibatch   2700  | loss  0.09 | err rate  3.12%\n",
      "Minibatch   2800  | loss  0.05 | err rate  2.34%\n",
      "Minibatch   2900  | loss  0.04 | err rate  0.78%\n",
      "Minibatch   3000  | loss  0.05 | err rate  1.56%\n",
      "Minibatch   3100  | loss  0.17 | err rate  3.91%\n",
      "----------------------------------------------------------\n",
      "After epoch  4 | valid err rate:  1.75% | doing   7 epochs\n",
      "----------------------------------------------------------\n",
      "Minibatch   3200  | loss  0.03 | err rate  0.78%\n",
      "Minibatch   3300  | loss  0.12 | err rate  2.34%\n",
      "Minibatch   3400  | loss  0.04 | err rate  1.56%\n",
      "Minibatch   3500  | loss  0.07 | err rate  1.56%\n",
      "Minibatch   3600  | loss  0.01 | err rate  0.00%\n",
      "Minibatch   3700  | loss  0.05 | err rate  1.56%\n",
      "Minibatch   3800  | loss  0.08 | err rate  2.34%\n",
      "Minibatch   3900  | loss  0.06 | err rate  0.78%\n",
      "----------------------------------------------------------\n",
      "After epoch  5 | valid err rate:  1.67% | doing   8 epochs\n",
      "----------------------------------------------------------\n",
      "Minibatch   4000  | loss  0.03 | err rate  0.00%\n",
      "Minibatch   4100  | loss  0.05 | err rate  1.56%\n",
      "Minibatch   4200  | loss  0.05 | err rate  1.56%\n",
      "Minibatch   4300  | loss  0.07 | err rate  2.34%\n",
      "Minibatch   4400  | loss  0.17 | err rate  2.34%\n",
      "Minibatch   4500  | loss  0.03 | err rate  1.56%\n",
      "Minibatch   4600  | loss  0.06 | err rate  3.12%\n",
      "----------------------------------------------------------\n",
      "After epoch  6 | valid err rate:  1.60% | doing  10 epochs\n",
      "----------------------------------------------------------\n",
      "Minibatch   4700  | loss  0.07 | err rate  2.34%\n",
      "Minibatch   4800  | loss  0.05 | err rate  1.56%\n",
      "Minibatch   4900  | loss  0.04 | err rate  0.78%\n",
      "Minibatch   5000  | loss  0.01 | err rate  0.00%\n",
      "Minibatch   5100  | loss  0.04 | err rate  1.56%\n",
      "Minibatch   5200  | loss  0.03 | err rate  0.78%\n",
      "Minibatch   5300  | loss  0.04 | err rate  2.34%\n",
      "Minibatch   5400  | loss  0.01 | err rate  0.00%\n",
      "----------------------------------------------------------\n",
      "After epoch  7 | valid err rate:  1.49% | doing  11 epochs\n",
      "----------------------------------------------------------\n",
      "Minibatch   5500  | loss  0.02 | err rate  0.00%\n",
      "Minibatch   5600  | loss  0.02 | err rate  0.78%\n",
      "Minibatch   5700  | loss  0.07 | err rate  1.56%\n",
      "Minibatch   5800  | loss  0.04 | err rate  1.56%\n",
      "Minibatch   5900  | loss  0.06 | err rate  1.56%\n",
      "Minibatch   6000  | loss  0.01 | err rate  0.00%\n",
      "Minibatch   6100  | loss  0.06 | err rate  1.56%\n",
      "Minibatch   6200  | loss  0.03 | err rate  0.78%\n",
      "----------------------------------------------------------\n",
      "After epoch  8 | valid err rate:  1.46% | doing  13 epochs\n",
      "----------------------------------------------------------\n",
      "Minibatch   6300  | loss  0.06 | err rate  2.34%\n",
      "Minibatch   6400  | loss  0.07 | err rate  2.34%\n",
      "Minibatch   6500  | loss  0.02 | err rate  1.56%\n",
      "Minibatch   6600  | loss  0.02 | err rate  1.56%\n",
      "Minibatch   6700  | loss  0.03 | err rate  0.78%\n",
      "Minibatch   6800  | loss  0.03 | err rate  1.56%\n",
      "Minibatch   6900  | loss  0.06 | err rate  2.34%\n",
      "Minibatch   7000  | loss  0.04 | err rate  0.78%\n",
      "----------------------------------------------------------\n",
      "After epoch  9 | valid err rate:  1.47% | doing  13 epochs\n",
      "----------------------------------------------------------\n",
      "Minibatch   7100  | loss  0.01 | err rate  0.00%\n",
      "Minibatch   7200  | loss  0.06 | err rate  1.56%\n",
      "Minibatch   7300  | loss  0.07 | err rate  1.56%\n",
      "Minibatch   7400  | loss  0.01 | err rate  0.00%\n",
      "Minibatch   7500  | loss  0.02 | err rate  0.78%\n",
      "Minibatch   7600  | loss  0.04 | err rate  0.78%\n",
      "Minibatch   7700  | loss  0.03 | err rate  0.78%\n",
      "Minibatch   7800  | loss  0.03 | err rate  0.78%\n",
      "----------------------------------------------------------\n",
      "After epoch 10 | valid err rate:  1.49% | doing  13 epochs\n",
      "----------------------------------------------------------\n",
      "Minibatch   7900  | loss  0.01 | err rate  0.78%\n",
      "Minibatch   8000  | loss  0.00 | err rate  0.00%\n",
      "Minibatch   8100  | loss  0.06 | err rate  0.78%\n",
      "Minibatch   8200  | loss  0.07 | err rate  2.34%\n",
      "Minibatch   8300  | loss  0.04 | err rate  1.56%\n",
      "Minibatch   8400  | loss  0.05 | err rate  2.34%\n",
      "Minibatch   8500  | loss  0.06 | err rate  3.12%\n",
      "Minibatch   8600  | loss  0.06 | err rate  1.56%\n",
      "----------------------------------------------------------\n",
      "After epoch 11 | valid err rate:  1.50% | doing  13 epochs\n",
      "----------------------------------------------------------\n",
      "Minibatch   8700  | loss  0.05 | err rate  2.34%\n",
      "Minibatch   8800  | loss  0.02 | err rate  0.00%\n",
      "Minibatch   8900  | loss  0.07 | err rate  2.34%\n",
      "Minibatch   9000  | loss  0.02 | err rate  0.78%\n",
      "Minibatch   9100  | loss  0.03 | err rate  0.78%\n",
      "Minibatch   9200  | loss  0.04 | err rate  1.56%\n",
      "Minibatch   9300  | loss  0.02 | err rate  0.00%\n",
      "----------------------------------------------------------\n",
      "After epoch 12 | valid err rate:  1.47% | doing  13 epochs\n",
      "----------------------------------------------------------\n",
      "Minibatch   9400  | loss  0.01 | err rate  0.00%\n",
      "Minibatch   9500  | loss  0.07 | err rate  2.34%\n",
      "Minibatch   9600  | loss  0.04 | err rate  1.56%\n",
      "Minibatch   9700  | loss  0.06 | err rate  1.56%\n",
      "Minibatch   9800  | loss  0.01 | err rate  0.78%\n",
      "Minibatch   9900  | loss  0.03 | err rate  1.56%\n",
      "Minibatch  10000  | loss  0.04 | err rate  1.56%\n",
      "Minibatch  10100  | loss  0.01 | err rate  0.00%\n",
      "----------------------------------------------------------\n",
      "After epoch 13 | valid err rate:  1.44% | doing  20 epochs\n",
      "----------------------------------------------------------\n",
      "Minibatch  10200  | loss  0.02 | err rate  0.78%\n",
      "Minibatch  10300  | loss  0.01 | err rate  0.78%\n",
      "Minibatch  10400  | loss  0.07 | err rate  2.34%\n",
      "Minibatch  10500  | loss  0.07 | err rate  0.78%\n",
      "Minibatch  10600  | loss  0.01 | err rate  0.00%\n",
      "Minibatch  10700  | loss  0.05 | err rate  1.56%\n",
      "Minibatch  10800  | loss  0.05 | err rate  1.56%\n",
      "Minibatch  10900  | loss  0.04 | err rate  1.56%\n",
      "----------------------------------------------------------\n",
      "After epoch 14 | valid err rate:  1.43% | doing  22 epochs\n",
      "----------------------------------------------------------\n",
      "Minibatch  11000  | loss  0.08 | err rate  2.34%\n",
      "Minibatch  11100  | loss  0.02 | err rate  0.78%\n",
      "Minibatch  11200  | loss  0.04 | err rate  1.56%\n",
      "Minibatch  11300  | loss  0.07 | err rate  1.56%\n",
      "Minibatch  11400  | loss  0.10 | err rate  1.56%\n",
      "Minibatch  11500  | loss  0.04 | err rate  1.56%\n",
      "Minibatch  11600  | loss  0.08 | err rate  3.12%\n",
      "Minibatch  11700  | loss  0.04 | err rate  1.56%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "After epoch 15 | valid err rate:  1.47% | doing  22 epochs\n",
      "----------------------------------------------------------\n",
      "Minibatch  11800  | loss  0.04 | err rate  0.78%\n",
      "Minibatch  11900  | loss  0.03 | err rate  1.56%\n",
      "Minibatch  12000  | loss  0.07 | err rate  3.91%\n",
      "Minibatch  12100  | loss  0.01 | err rate  0.78%\n",
      "Minibatch  12200  | loss  0.01 | err rate  0.00%\n",
      "Minibatch  12300  | loss  0.02 | err rate  0.78%\n",
      "Minibatch  12400  | loss  0.13 | err rate  1.56%\n",
      "Minibatch  12500  | loss  0.02 | err rate  0.00%\n",
      "----------------------------------------------------------\n",
      "After epoch 16 | valid err rate:  1.47% | doing  22 epochs\n",
      "----------------------------------------------------------\n",
      "Minibatch  12600  | loss  0.06 | err rate  2.34%\n",
      "Minibatch  12700  | loss  0.05 | err rate  1.56%\n",
      "Minibatch  12800  | loss  0.01 | err rate  0.00%\n",
      "Minibatch  12900  | loss  0.04 | err rate  1.56%\n",
      "Minibatch  13000  | loss  0.08 | err rate  3.12%\n",
      "Minibatch  13100  | loss  0.03 | err rate  0.78%\n",
      "Minibatch  13200  | loss  0.01 | err rate  0.00%\n",
      "----------------------------------------------------------\n",
      "After epoch 17 | valid err rate:  1.45% | doing  22 epochs\n",
      "----------------------------------------------------------\n",
      "Minibatch  13300  | loss  0.07 | err rate  0.78%\n",
      "Minibatch  13400  | loss  0.04 | err rate  1.56%\n",
      "Minibatch  13500  | loss  0.03 | err rate  0.78%\n",
      "Minibatch  13600  | loss  0.04 | err rate  2.34%\n",
      "Minibatch  13700  | loss  0.02 | err rate  0.78%\n",
      "Minibatch  13800  | loss  0.04 | err rate  1.56%\n",
      "Minibatch  13900  | loss  0.04 | err rate  0.78%\n",
      "Minibatch  14000  | loss  0.01 | err rate  0.00%\n",
      "----------------------------------------------------------\n",
      "After epoch 18 | valid err rate:  1.45% | doing  22 epochs\n",
      "----------------------------------------------------------\n",
      "Minibatch  14100  | loss  0.04 | err rate  3.12%\n",
      "Minibatch  14200  | loss  0.07 | err rate  3.12%\n",
      "Minibatch  14300  | loss  0.01 | err rate  0.00%\n",
      "Minibatch  14400  | loss  0.02 | err rate  0.78%\n",
      "Minibatch  14500  | loss  0.03 | err rate  0.78%\n",
      "Minibatch  14600  | loss  0.03 | err rate  1.56%\n",
      "Minibatch  14700  | loss  0.04 | err rate  0.78%\n",
      "Minibatch  14800  | loss  0.03 | err rate  0.78%\n",
      "----------------------------------------------------------\n",
      "After epoch 19 | valid err rate:  1.43% | doing  22 epochs\n",
      "----------------------------------------------------------\n",
      "Minibatch  14900  | loss  0.06 | err rate  2.34%\n",
      "Minibatch  15000  | loss  0.06 | err rate  0.78%\n",
      "Minibatch  15100  | loss  0.01 | err rate  0.78%\n",
      "Minibatch  15200  | loss  0.05 | err rate  3.12%\n",
      "Minibatch  15300  | loss  0.01 | err rate  0.00%\n",
      "Minibatch  15400  | loss  0.04 | err rate  0.78%\n",
      "Minibatch  15500  | loss  0.02 | err rate  0.00%\n",
      "Minibatch  15600  | loss  0.02 | err rate  0.78%\n",
      "----------------------------------------------------------\n",
      "After epoch 20 | valid err rate:  1.44% | doing  22 epochs\n",
      "----------------------------------------------------------\n",
      "Minibatch  15700  | loss  0.06 | err rate  1.56%\n",
      "Minibatch  15800  | loss  0.04 | err rate  0.78%\n",
      "Minibatch  15900  | loss  0.10 | err rate  3.91%\n",
      "Minibatch  16000  | loss  0.03 | err rate  0.78%\n",
      "Minibatch  16100  | loss  0.02 | err rate  0.00%\n",
      "Minibatch  16200  | loss  0.04 | err rate  0.78%\n",
      "Minibatch  16300  | loss  0.04 | err rate  1.56%\n",
      "Minibatch  16400  | loss  0.07 | err rate  2.34%\n",
      "----------------------------------------------------------\n",
      "After epoch 21 | valid err rate:  1.44% | doing  22 epochs\n",
      "----------------------------------------------------------\n",
      "Minibatch  16500  | loss  0.04 | err rate  1.56%\n",
      "Minibatch  16600  | loss  0.01 | err rate  0.78%\n",
      "Minibatch  16700  | loss  0.02 | err rate  0.00%\n",
      "Minibatch  16800  | loss  0.03 | err rate  0.78%\n",
      "Minibatch  16900  | loss  0.01 | err rate  0.00%\n",
      "Minibatch  17000  | loss  0.03 | err rate  0.78%\n",
      "Minibatch  17100  | loss  0.02 | err rate  0.78%\n",
      "Minibatch  17200  | loss  0.01 | err rate  0.78%\n",
      "----------------------------------------------------------\n",
      "After epoch 22 | valid err rate:  1.44% | doing  22 epochs\n",
      "----------------------------------------------------------\n",
      "\n",
      "Loading best params on validation set (epoch 14)\n",
      "\n",
      "Test error rate: 1.34%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA64AAAD8CAYAAAB3qPkTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3XlYlFX7B/DvEVHMBTdyLyxNRURUUMvcci1TWyzb7LVF\n2st2yt603rfyp2amWWab1ltqabmE5griggoobojiAggom7KDbOf3xwzj7Mw+D/D9XJeXM896zzDw\nzP2cc+4jpJQgIiIiIiIiUqoG7g6AiIiIiIiIyBwmrkRERERERKRoTFyJiIiIiIhI0Zi4EhERERER\nkaIxcSUiIiIiIiJFY+JKREREREREisbElYiIqJYSQnQRQoQLIeKFECeFEK+pl7cWQmwXQiSq/29l\nYv/xQojTQoizQohQ10ZPRERkOcF5XImIiGonIUQHAB2klIeFEM0BxAK4D8B0AFeklHPVCWkrKeW7\nevt6ADgDYAyAVADRAB6VUsa78jUQERFZgi2uREREtZSU8pKU8rD6cQGAUwA6AZgMYKV6s5VQJbP6\nBgI4K6U8L6UsA7BavR8REZHiNHR3AOa0bdtW+vr6ujsMIiKqI2JjY7OllD7ujsMZhBC+APoBOAig\nnZTyknrVZQDtjOzSCcBFreepAAaZOHYIgBAAaNq06YCePXs6JObjaXmax306eTvkmEREVLtYem1W\ndOLq6+uLmJgYd4dBRER1hBAi2d0xOIMQohmAdQBmSinzhRCadVJKKYSwa1yQlHI5gOUAEBQUJB11\nbfYNDdM8jpk7wSHHJCKi2sXSazO7ChMREdViQghPqJLWX6WUf6oXZ6jHv1aPg800smsagC5azzur\nlxERESmOIhNXIcREIcTyvLy8mjcmIiKqp4SqafUHAKeklAu1Vm0E8C/1438B2GBk92gA3YUQXYUQ\njQA8ot6PiIhIcRSZuEopN0kpQ7y9Od6FiIjIjCEApgG4SwgRp/53D4C5AMYIIRIBjFY/hxCioxBi\nMwBIKSsAvAxgK1RFnX6XUp50x4sgIiKqiaLHuBIRuUt5eTlSU1NRWlrq7lDIBl5eXujcuTM8PT3d\nHYpTSSn3AhAmVo8ysn06gHu0nm8GsNk50RFRXcbrJFnL3mszE1ciIiNSU1PRvHlz+Pr6QrvQDSmf\nlBI5OTlITU1F165d3R0OEVGdxOskWcMR12ZFdhUmInK30tJStGnThhfjWkgIgTZt2rAVgIjIiXid\nJGs44trMxJWIyARejGsv/uyIiJyPf2vJGvZ+Xup84nrwfA58Q8NwMp0ViomIiIiIiGqjOp+4bjia\nDgCYsHivmyMhIrJcUlIS/P39rdpnxYoVSE9Pr3Gbl19+ucZjLVq0CMXFxVadHwA+/PBD7Nixw+Lt\nIyIicO+991p9HiIiqt/qy3VSqSIiIrB//36XnrPOJ66f3t8HANDIo86/VCKq5yy5IFvK3AW5srLS\n5H4ff/wxRo8e7ZAYiIiIHKmuXicrKip0nkspUVVVZdG+5mLVP642Jq5OMrpXO3S7sZm7wyAiskpF\nRQUef/xx9OrVC1OmTNFcID/++GMEBwfD398fISEhkFJi7dq1iImJweOPP47AwECUlJQgOjoad9xx\nB/r27YuBAweioKAAAJCeno7x48eje/fueOeddwzOu3jxYqSnp2PkyJEYOXIkAKBZs2Z488030bdv\nX0RFRRmNAQCmT5+OtWvXAgB8fX0xe/Zs9O/fH3369EFCQoLZ13vlyhXcd999CAgIwODBg3Hs2DEA\nwO7duxEYGIjAwED069cPBQUFuHTpEoYNG4bAwED4+/tjz549jnnTiYio1qgP18nKykq8/fbbCA4O\nRkBAAL799lsAqsRx6NChmDRpEvz8/JCUlIQePXrgySefhL+/Py5evIhVq1ahT58+8Pf3x7vvvqs5\npn6s2kaMGIGZM2ciKCgIX375JTZt2oRBgwahX79+GD16NDIyMpCUlIRly5bhiy++QGBgIPbs2YOs\nrCw8+OCDCA4ORnBwMPbt22fvj9dAvZgOp7FnA1yrMH03gYjInI82nUR8er5Dj+nXsQVmT+xtdpvT\np0/jhx9+wJAhQ/D000/j66+/xltvvYWXX34ZH374IQBg2rRp+PvvvzFlyhR89dVXWLBgAYKCglBW\nVoapU6dizZo1CA4ORn5+Ppo0aQIAiIuLw5EjR9C4cWP06NEDr7zyCrp06aI576uvvoqFCxciPDwc\nbdu2BQAUFRVh0KBB+Pzzz1Xx+/kZxDBx4kSD19C2bVscPnwYX3/9NRYsWIDvv//e5OudPXs2+vXr\nh/Xr12PXrl148sknERcXhwULFmDp0qUYMmQICgsL4eXlheXLl2PcuHGYNWsWKisrbequRcoipWSh\nF6JaitdJ510nf/jhB3h7eyM6OhrXrl3DkCFDMHbsWADA4cOHceLECXTt2hVJSUlITEzEypUrMXjw\nYKSnp+Pdd99FbGwsWrVqhbFjx2L9+vW47777DGLVV1ZWhpiYGADA1atXceDAAQgh8P3332PevHn4\n/PPP8fzzz6NZs2Z46623AACPPfYYXn/9ddx5551ISUnBuHHjcOrUKbM/P2spssVVCDFRCLE8L88x\nBZUaN2yAaxWWNZcTESlFly5dMGTIEADAE088gb17VWP1w8PDMWjQIPTp0we7du3CyZMnDfY9ffo0\nOnTogODgYABAixYt0LCh6l7lqFGj4O3tDS8vL/j5+SE5ObnGWDw8PPDggw9qnlsSAwA88MADAIAB\nAwYgKSnJ7Dn27t2LadOmAQDuuusu5OTkID8/H0OGDMEbb7yBxYsXIzc3Fw0bNkRwcDB++uknzJkz\nB8ePH0fz5s1rfA2kbLxOE5G16sN1ctu2bfj5558RGBiIQYMGIScnB4mJiQCAgQMH6syJevPNN2Pw\n4MEAgOjoaIwYMQI+Pj5o2LAhHn/8cURGRhqNVd/UqVM1j1NTUzFu3Dj06dMH8+fPN/k6duzYgZdf\nfhmBgYGYNGkS8vPzUVhYaPIctlBki6uUchOATUFBQTMccbzGDT14QSQim9V0x9dZ9FufhBAoLS3F\niy++iJiYGHTp0gVz5syxek60xo0bax57eHiYHcNSzcvLCx4eHgBgVQzV57L0PMaEhoZiwoQJ2Lx5\nM4YMGYKtW7di2LBhiIyMRFhYGKZPn4433ngDTz75pE3HJyIi+/A66bzrpJQSS5Yswbhx43SWR0RE\noGnTpjrL9J9bEqsx2sd55ZVX8MYbb2DSpEmIiIjAnDlzjO5TVVWFAwcOwMvLy6IYbKHIFldHa9yw\nAa6Vs6swEdUuKSkpmrEnv/32G+68807Nha9t27YoLCzUjJMBgObNm2vG5/To0QOXLl1CdHQ0AKCg\noMCqxFH7WPrMxWCPoUOH4tdffwWguiC3bdsWLVq0wLlz59CnTx+8++67CA4ORkJCApKTk9GuXTvM\nmDEDzz77LA4fPuyQGIiIqPaoD9fJcePG4ZtvvkF5eTkA4MyZMygqKqpxv4EDB2L37t3Izs5GZWUl\nVq1aheHDh1t9/ry8PHTq1AkAsHLlSs1y/dc/duxYLFmyRPM8Li7O6nPVpH4krp7sKkxEtU+PHj2w\ndOlS9OrVC1evXsULL7yAli1bYsaMGfD398e4ceM0XZwAVcGH559/HoGBgaisrMSaNWvwyiuvoG/f\nvhgzZoxVd5xDQkIwfvx4TdEJbeZisMecOXMQGxuLgIAAhIaGai6QixYtgr+/PwICAuDp6Ym7774b\nERER6Nu3L/r164c1a9bgtddec0gMRERUe9SH6+Szzz4LPz8/9O/fH/7+/njuuecsSrA7dOiAuXPn\nYuTIkejbty8GDBiAyZMnW33+OXPm4KGHHsKAAQM043kBYOLEifjrr780xZkWL16MmJgYBAQEwM/P\nD8uWLbP6XDUR1RWulCgoKEhWDwy2x8LtZ7B4ZyIufHYPCz8QkUVOnTqFXr16uTsMsoOxn6EQIlZK\nGeSmkOoER12bAcA3NEzzOOE/4+HlabrrGhEpC6+TZAt7rs31o8W1oeplllWy1ZWIiIiIiKi2qReJ\nq6eHqpW1olK5rctERERERERkXD1JXFUvs5wtrkRkBSUPpSDz+LOrndJyS5BXXO7uMIiISIHqVeLK\nrsJEZCkvLy/k5OQwAaqFpJTIyclxakl+crwvdyZiyNxdGL4g3N2hEBGRAilyHldHa6RpceUXUCKy\nTOfOnZGamoqsrCx3h0I28PLyQufOnd0dBllh/7kcAEAuW1yJiMiIepG4ejZUjXEt55Q4RGQhT09P\ndO3a1d1hENVICPEjgHsBZEop/dXL1gDood6kJYBcKWWgkX2TABQAqARQwYrLRESkVPWiq/CexGwA\nQNjxS26OhIiIyOFWABivvUBKOVVKGahOVtcB+NPM/iPV2zJpJaI6rVmzZgCA9PR0TJkyxeg2I0aM\nQE1Tfi1atAjFxcWa5/fccw9yc3MdF6jCrFixAunp6e4Oo34krokZhQCAmKQrbo6EiIjIsaSUkQCM\nXuCEavLyhwGscmlQREQK1rFjR6xdu9bm/fUT182bN6Nly5aOCK1GFRUVZp9bup++yspKk+uYuLpQ\nI/U8rtfYVZiIiOqXoQAypJSJJtZLADuEELFCiBAXxkVEZJfQ0FAsXbpU83zOnDlYsGABCgsLMWrU\nKPTv3x99+vTBhg0bDPZNSkqCv78/AKCkpASPPPIIevXqhfvvvx8lJSWa7V544QUEBQWhd+/emD17\nNgBg8eLFSE9Px8iRIzFy5EgAgK+vL7KzVT08Fy5cCH9/f/j7+2PRokWa8/Xq1QszZsxA7969MXbs\nWJ3zVMvKysKDDz6I4OBgBAcHY9++fZrXNm3aNAwZMgTTpk3DihUrMGnSJNx1110YNWoUpJR4++23\n4e/vjz59+mDNmjUAgIiICAwdOhSTJk2Cn5+fwfmaNWuGN998E3379kVUVBQ+/vhjBAcHw9/fHyEh\nIZBSYu3atYiJicHjjz+OwMBAlJSUIDY2FsOHD8eAAQMwbtw4XLrkml6t9WKMa78uLRGbfBW3+DR1\ndyhERESu9CjMt7beKaVME0LcCGC7ECJB3YKrQ53UhgDATTfd5JxIiaj2mjkTiItz7DEDAwF14mfM\n1KlTMXPmTLz00ksAgN9//x1bt26Fl5cX/vrrL7Ro0QLZ2dkYPHgwJk2aBFUHFEPffPMNbrjhBpw6\ndQrHjh1D//79Nes++eQTtG7dGpWVlRg1ahSOHTuGV199FQsXLkR4eDjatm2rc6zY2Fj89NNPOHjw\nIKSUGDRoEIYPH45WrVohMTERq1atwnfffYeHH34Y69atwxNPPKGz/2uvvYbXX38dd955J1JSUjBu\n3DicOnUKABAfH4+9e/eiSZMmWLFiBQ4fPoxjx46hdevWWLduHeLi4nD06FFkZ2cjODgYw4YNAwAc\nPnwYJ06cMFq3o6ioCIMGDcLnn38OAPDz88OHH34IAJg2bRr+/vtvTJkyBV999RUWLFiAoKAglJeX\n45VXXsGGDRvg4+ODNWvWYNasWfjxxx/N/jgdwWWJqxCiKYCvAZQBiJBS/uqqc9/XrxO+33sBQ7v7\nuOqUREREbiWEaAjgAQADTG0jpUxT/58phPgLwEAABomrlHI5gOUAEBQUxBL9ROR2/fr1Q2ZmJtLT\n05GVlYVWrVqhS5cuKC8vx/vvv4/IyEg0aNAAaWlpyMjIQPv27Y0eJzIyEq+++ioAICAgAAEBAZp1\nv//+O5YvX46KigpcunQJ8fHxOuv17d27F/fffz+aNlU1lj3wwAPYs2cPJk2ahK5duyIwUFUjb8CA\nAUhKSjLYf8eOHYiPj9c8z8/PR2GhasjjpEmT0KRJE826MWPGoHXr1przPvroo/Dw8EC7du0wfPhw\nREdHo0WLFhg4cKDJYpMeHh548MEHNc/Dw8Mxb948FBcX48qVK+jduzcmTpyos8/p06dx4sQJjBkz\nBoCqi3GHDh1MvieOZFfiaqySoXr5eABfAvAA8L2Uci5UF8+1UspN6mqHLktcPRqo7rBUVfFaS0RE\n9cZoAAlSylRjK9U3lBtIKQvUj8cC+NiVAWo7erHuFjYhqvPMtIw600MPPYS1a9fi8uXLmDp1KgDg\n119/RVZWFmJjY+Hp6QlfX1+UlpZafewLFy5gwYIFiI6ORqtWrTB9+nSbjlOtcePGmsceHh5GuwpX\nVVXhwIEDRuchr06GTT03xdx2Xl5e8PDwAACUlpbixRdfRExMDLp06YI5c+YYfb1SSvTu3RtRUVEW\nnd+R7B3jugJ6lQyFEB4AlgK4G4AfgEeFEH4AOgO4qN7M9OhfJ2ioTlwrJRNXIiKqW4QQqwBEAegh\nhEgVQjyjXvUI9LoJCyE6CiE2q5+2A7BXCHEUwCEAYVLKf1wVNxGRvaZOnYrVq1dj7dq1eOihhwAA\neXl5uPHGG+Hp6Ynw8HAkJyebPcawYcPw22+/AQBOnDiBY8eOAVC1djZt2hTe3t7IyMjAli1bNPs0\nb94cBQUFBscaOnQo1q9fj+LiYhQVFeGvv/7C0KFDLX49Y8eOxZIlSzTP4yzsfj106FCsWbMGlZWV\nyMrKQmRkJAYOHGjxeQFoktS2bduisLBQp3iV9uvt0aMHsrKyNIlreXk5Tp48adW5bGVXi6uUMlII\n4au3eCCAs1LK8wAghFgNYDKAVKiS1zi4uChUA3XiWl7J4kxERFS3SCkfNbF8upFl6QDuUT8+D6Cv\nU4MjInKi3r17o6CgAJ06ddJ0V3388ccxceJE9OnTB0FBQejZs6fZY7zwwgt46qmn0KtXL/Tq1QsD\nBqhGV/Tt2xf9+vVDz5490aVLFwwZMkSzT0hICMaPH4+OHTsiPDxcs7x///6YPn26Jml89tln0a9f\nP6Pdgo1ZvHgxXnrpJQQEBKCiogLDhg3DsmXLatzv/vvvR1RUFPr27QshBObNm4f27dsjISHBovMC\nQMuWLTFjxgz4+/ujffv2CA4O1qybPn06nn/+eTRp0gRRUVFYu3YtXn31VeTl5aGiogIzZ85E7969\nLT6XrYS0sxVSnbj+rTXp+RQA46WUz6qfTwMwCMC7AL4CUApgr6kxrnoFIAbUdJfEEsk5RRg+PwIA\nkDR3gt3HIyKi2kkIEcv5Su0TFBQka5rj0FK+oWFGl/NaTaR8p06dQq9evdwdBtUyxj43ll6bXVac\nSUpZBOApC7ZzeAGIBiaqiBEREREREZHyOaPLbhqALlrPO6uXWUwIMVEIsTwvL88hATX0YOJKRERE\nRERUWzkjcY0G0F0I0VUI0Qiq4hAbrTmAlHKTlDLE29vbIQF5sMWViIiIiMih7B1ySPWLvZ8XuxJX\nY5UMpZQVAF4GsBXAKQC/SyldU2rKhOriTADwywH7x8wSEREREdVnXl5eyMnJYfJKFpFSIicnx+hU\nP5ayt6qwqUqGmwFsNrbOHaq0fqH+vf4Epg2+2Y3REBERERHVbp07d0ZqaiqysrLcHQrVEl5eXujc\nubPN+7usOJM1hBATAUzs1q2bQ47XsIFLZ98hIiIiIqrTPD090bVrV3eHQfWIIjM6R49xbd20kUOO\nQ0RERERERK6nyMSViIiIiIiIqJoiE1dHT4djzDMrovH5ttNOOz4RERERERE5hiITV0d3FTZmZ0Im\nluw667TjExERERERkWMoMnF1tg/WH3d3CERERERERGShepm4/u9AirtDICIiIiIiIgspMnF1xRhX\nIiIiIiIiqh0Umbi6Yoyrtqoqiaoq6ZJzERERkXm7EjLcHQIRESmMIhNXVwv4aBtGfh7h7jCIiIgI\nwNMrYtwdAhERKUxDdwegBIXXKlB4rcLdYRAREREREZER9b7F1Tc0zN0hEBERERERkRmKTFxZnImI\niIiIiIiqKTJxdXVxJiIiotpKCPGjECJTCHFCa9kcIUSaECJO/e8eE/uOF0KcFkKcFUKEui5qIiIi\n6ygycXWX+PR8xCZfcXcYRERE1lgBYLyR5V9IKQPV/zbrrxRCeABYCuBuAH4AHhVC+Dk1UiIiIhsx\ncdVyz+I9ePCbKHeHQUREZDEpZSQAW+66DgRwVkp5XkpZBmA1gMkODY6IiMhB6k3iOv0OX4u3Lauo\nQhGrDBMRUe32ihDimLorcSsj6zsBuKj1PFW9zIAQIkQIESOEiMnKynJGrERERGbVm8RVCMu3ve2D\nLeg9e6vzgiEiInKubwDcAiAQwCUAn9tzMCnlcillkJQyyMfHxxHxERERWUWRiaszqgqP6tnO6n2e\n/PEQSssrHRYDERGRK0gpM6SUlVLKKgDfQdUtWF8agC5azzurlxERESmOIhNXZ1QVvrN7W6v3iTyT\nhce/P+iwGIiIiFxBCNFB6+n9AE4Y2SwaQHchRFchRCMAjwDY6Ir4iIiIrNXQ3QEoXWzyVSzemYgG\nAjiWmofJgZ0wIaBDzTsSERG5gBBiFYARANoKIVIBzAYwQggRCEACSALwnHrbjgC+l1LeI6WsEEK8\nDGArAA8AP0opT7rhJRAREdWIiasFFm4/o3m8LT4Dw24bi+Zenm6MiIiISEVK+aiRxT+Y2DYdwD1a\nzzcDMJgqh4iISGkU2VVY6SqrpM7zd9YexZiFuwEAP+27gMMpV90RFhERERERUZ1UrxLXkGG3OOQ4\nUjdvxe8xqUjMLAQAfLQpHg98vV9rWwmpvwMRERERERFZrF4lrnd2s75AkzGWpKF/H0tH3MVcPLL8\nALq+V3MvrLOZhYhPz7c/OCIiIiIiojqmXo1xdVS758HzORjZ80ZICayPMz5zwMu/HbHqmKPVXY1X\nzRiM229tY3eMREREREREdYUiE1chxEQAE7t16+bQ41Y5qMvuC78exv39OmFHfAYKrlVYtM/K/Uko\nr6zCiB4+6HZjc5PbbYhL00lcpZQ4n12EW32a2R03ERERERFRbaTIrsLOmMdVdWDHHeqvI2kWJ60A\n8O3uc/hv2Cnct3R/zRtrWbE/CaM+321xwaeEy/nYfzbbqnMQEREREREpmSITV2eRjsxcbVRcZnmy\nCwBxF3MBACk5xTrL10SnIPx0psH24xftwWPfH7Q9QCIiIiIiIoVRZFfh2uqIjdPg+IaGWb3Pu+uO\nAwCS5k6w6ZykDGczC1FRVYWe7Vu4OxQiIiIiIsWqXy2uTm5wvf9r67oBm3M2sxBf7kjUPHdEa3Fa\nbgm+iThn9fQ8ucVl+Ha39ftRzUYv3I3xi/a4OwwiIiIiIkWrV4lrlRvzrgr1yasksP9cNi7llWDT\n0XSTra2PfncAX+w4g/yScpvOV1peiWk/HMS5rELNspCfY/B//yQg5UqxmT0NzfrrBD7bkoCo8zk2\nxUJERERERGSPetVV2J0thpkF1zSPH/vuIFp4NcSAm1sZbJeUU4Rh88KRpbU9YH1r8f5z2diTmI3/\n/h2Pn54aCAAoUheTsjaBzy9VJc/llWxxJWXLLS7DDY0aolHDenVPjoiIiKjOq1ff7irc2eSqJ7+0\nAmcyCg2WHzh/RadFVAhh13mMvWJ2+aW6KvDj7Zjxc4zLznc2swAXrezBQESWOZtZiMz8UneHQURE\nClGvEteb29zg7hB0pOWW1LiNpWmrlBJTv43S2k+ol2sdy94kmAmvQ8Sn5+PvY+k27Xs45SqqFHQD\nRol2n8ly2blGL4zE0HnhLjtfXZVdeM3qiutU941euBsDP93p7jCIiEgh6lXi2rujN47PGevuMKxS\nnWv+djDFbKJbcK0CBy9c0dpR9Z/RFlerY7A94S0pq8SiHWdQXlll8zHqmnsW78HLvx2xer/957Lx\nwNf7sXzPeSdEReQ+Qf/dgclf7XN3GERERKRg9SpxBYDmXp7uDsEmMclX8YSZ+Vkb6CWX1c+qW0mL\nrlWgoNR1LRr7z2ZjR3wGFu9KxKIdiVgTfdFp5yqrqMKZjAKHHa+qSiom0V5/JA3jF0UCANJzVV3m\nHPlaiZQiMdNw6AQRERFRNUUmrkKIiUKI5Xl5ee4Oxe20CyJdyC4yWJ9VcA23fbAFxy7m6iyf/lO0\nzvPes7ciu1C34JO1rGmpfez7g3j25xiUlFUCUCWXlqqqkrhSVGbx9v/5Ox5jv4hEugVdry3xwq+x\n6D5ri0OOZa+Za+KQcJmJKhERERHVb4pMXKWUm6SUId7e3u4Oxe30x+vtTczWeb7/XDbKKqrw474L\nRvc/kpJrsOzJHw5ZVPAiPbcET6+IRvE161pqrUk6jfl8+2n0/892ixPt6CRVF+mrxfadt9rWkxlm\n15/JKKjVBUNik6+g+6zNyLHzRoajXKuodHcIdUJ5ZRVGLojAtpOX3R0KERERkcMpMnEl0zYdTUdy\njmHL62UTiVThtQqDqXXScksw8NOdJhOG5Jwi+IaGYdJXe7ErIRMxyVc163xDw7BkZ6LZGPv/Z7vB\nMgmguKwCvqFhWH0oRbM87mKuQYK6TZ04WpoAV4/BlVL15f3ZldEGCb4jjf0iEoM/q70FQ77dfR7l\nlRLRSVdr3tjJ4tPz0eODf/DPiUsuO2eFQrqBO9rVojJcyC7CrPUnzG539GIufEPDjP4dISIiIlKq\nepm49ulUe1ty18RcxPD5EQbLT6Tlm9wn+JMdRpe/tirO6PLqVtrsQuOJ4+fbz9QQ5XXVY0U3Hk3H\nJHXxla8jzmnW37d0HyYt2auzj9XFo7Qeb4/PwI5TmXjiB9PjgR3BnYV9/zychrWxqS49Z0pOMd77\n87jDk77jaarP2q6ETIce15QL2UXoNmsLNsSlueR8SlT92XFk9eWcwmtYtvscK4+7iRDiRyFEphDi\nhNay+UKIBCHEMSHEX0KIlib2TRJCHBdCxAkhXDeXFBERkZXqZeL6x/O3uzsEh0nJsX0Oya3xtncp\nzC8th29oGH6JStIsO5meh5/0uiz/elDVunr0Yi7Omii+kp5nX7db7bpUlXVgqpjw05korKF79lt/\nHHXAmSx/r15bcwSrDqXgaKrhuPOyiiqUlteO7r4Jl1Q3eLYcZ3daQPWzy3VAF/u31x7D3C0JOHLR\ncGgCucQKAOP1lm0H4C+lDABwBsB7ZvYfKaUMlFIGOSk+IiIiu9XLxNXL08PdITiMNa2f+qRUddW1\nxWV1svkIFtH/AAAgAElEQVRzVLJm2YTFe/HRpnibjpeRX2p38iMl8HuMYfXilJxiXMpzTOEmZ7t4\npRhP/RSNt363LjG1pjCVsdmN/jlxCVHncizY2zDZHT4/HD3//Y/F53ek3w6mwDc0zKA7vKPkFZfj\nvT+Pa4qMOVtSdhFOpufhRFqeyTHe2+MzsDT8rMljWNPoOePnGAR+bNi131r5JeUAgIpKw5M/9dMh\nvPzbYauOJ6VETNIVtuBaSEoZCeCK3rJtUsrqO2AHAHR2eWAOsnhnIj7dfMrdYRARkZvVy8SVros8\nk4XKKokTadZVcHZEV0vtsa6DPt2JJ388ZHb74rIKFBlpiRSaOWsl9miNba2orIKUEsPmh+P2z3YZ\nPebuM1nYk2i8y+T3e85jTbQqxvzSciSamIbmj5iLmilrACDRjuJNRWWq16dfQdrcF/iYpCu4Y+4u\ns92H49Pz8dwvMVgeec5o8ann/3cYj353wOT++rluZZXUxHTJzhZz1fGAwZ/uNPm5KimrxM9RSQbv\nwx+xqhsVKVes63kgTbQ2f7wpHl9o3Qz6cmciVh1KwSqtz6oxhy5cQepV23s/VBuxIAITFu/FvUv2\nYtwXkUa3mfFzDOZvPW24wobplvW7C1dWSRSUluOlXw9jnfrzlJJTDN/QMOw8Zb5oGQA8uzLaoNdD\n+Oks/H3MujHMG4+mY8qyKKw7XH+7dDvY0wBMlUqXAHYIIWKFECGmDiCECBFCxAghYrKyHNfNfN0L\nNfeAWrj9DJZHcv5qIqL6jolrPbf6UAo2xKXh3iV7se3kZZSUVZpuzdH6Pro0/Jz+IquF/nlc5/mh\nC1dMbKniP3sres/earBc4HpxJm3dZm0x+VpWHVK11P3rx0OY9oPxhPm/Yafw7rrjeHpFNALmbMMY\nE4nE22uPIeFyAaSU6P3hPxjzRSRun2s8UTYlNrnmQkmm8pLT6oTa3DHe+D0OW09m4NPNCZplUqqS\nXu1uyUXXKvDen8dNdlWWEsgtLsOt72/Gd3t0v0g+/G2UTgJvjdLySlzOL8X7ep+JanO3nMKHG05i\nxynbxsJWVFZh9oYTJouYVftx3wV8qVV8rMrCFr+Hv43C0HnhBue0R44FxcmyC69dv6Fi5S+jsZf2\n/p/H0WfONoQdv4Q31d3Rj1xUfa7Wx6XXeMz80gq7ejf8djAFUkokq4dAsICU/YQQswBUAPjVxCZ3\nSikDAdwN4CUhxDBjG0kpl0spg6SUQT4+Pg6Lb8DNrR12LCIiqtuYuNZz6Xmlmgq8P+y9gH/9dAiJ\nJsainssyvtyRLmQXaVrV9BM17Yac9NwSlFdWobJKalpcjY2hXWOk6zAAs10t9ZkqHPT3Md0v8hVV\nEkXqLqXWjrXdfSYLUkrsVCdmxloEHd1pMr+0HFOWReHFX6934/xp3wWsOpSC5bvP6WwrtPoXZ6q7\n5f4Ro9vCe+jCFZ05Z+f9k4D9Z1WfrVdXHcFrq4/g9+iL2HTUMAEy1QJ6Ka8El/JKcKVY1RW1uMy6\nqZm2HL+EkJ9jcPDCFayMSra5K7sltBPByDNZ6DZrC446ecznyPkRBjdUjHUFN7Z+9saTBuuqW7At\nlVdcbrSLf3JOEVaYmKLLnPf/Oo719bhwlqMJIaYDuBfA49JEtw0pZZr6/0wAfwEY6LIAiYiIrMDE\nVe25Ybe4OwS3Oa7uJnzwwhWzrZ7/DTMcY3Q2sxBpuSXwN9ISaouRCyJwLut6K8t3kefhGxqm8+X4\nWGou7pi7C91nbcGYhbtxTF0waNFO8+N9zSU9v8dcxIXsIovnjgWAl387YvUYPHNdrLeezDDeBRSq\npMjR4/3KKlQtgjFJ13/m1fl2wuUClJRVoqyiCnkl5Zou2hKWj6H8OuIcHvteVd1549F0bIhLxzvr\njuGVVUc027y7TreFVehlXbd/tstkN29jr6XaF9vPQEqJF349jG3xGQYxVz/PK1EVGfvnhOOKNcUk\nXdF0e5+8dJ/DjmtMgfrnUlBarllmy8fkeGoezmYWWl0tu+/H2wy6mC/cdgbD50dgzqZ4+IaGWR3L\n62uOaqbq4hBX2wkhxgN4B8AkKaXRfuxCiKZCiObVjwGMBWB+PiUiIiI3YeKq1qppI3eH4DamWlgt\n9dqqIzVWwdWWcqXY4iTsE3VBjkGfXp83Vbtl9bzWWNCLVwy7KGqfpno6HgBIvaq77Ttrj2Hkggjc\nYUGSpG2sVmuXqZdU/SX8RFoeXlt9fQoi7YI/AkCWVtIsjHQMXhNtXWuYtiIjSXt1uMVGCg9ti8/A\na6uP4MFv9qPvR9t0WlI1MZpp2bO22rX2e1d0TTXf7497jbfYSSmx+fgllJZXavZ7dmW0zjZf7kzE\ngfPXE3JTsVZ/lr6NPGew7p4v92DF/iSTMUecNt4S/5PePp9vM34zwhrZhdcw+au9Jotw9ZmzTZN0\nWnPzpdrEr/Zi9MLdNsVWPX1WtT+PmG8x/efEJfiGhpmdp7l6KAJZRgixCkAUgB5CiFQhxDMAvgLQ\nHMB29VQ3y9TbdhRCbFbv2g7AXiHEUQCHAIRJKd1TaY2IiKgGDd0dgFLwzr7trElaq1nSHVA72cgr\nKTe9oRnaP1dT0/FoK7NyXKK5pF+7temfmUMNEsQf9pouNmKs62z8JcO5ekvLKxFtpJVcSonpP0Vj\n+h2+GNnzRlwtMnz/PjbSbVb7/YpJvmqQXFj6e/LQt/s1jzMsKFSl/bOuTrw+/vt6fNp55/8OJOPf\nG3S7uRYZSb61WwJr+vwcScnFS3qVb7Xfb2OJr6l5jvUt2XUWb47tAQCoqpL4Zvc5TLv9ZrTw8rRo\nfwBYF5uKo6l5+Hyb6V4F+mOOnSUpuwijF+5GhY1TTz3/P9X7nJhRgEG3tHFkaPWWlPJRI4t/MLFt\nOoB71I/PA+jrxNCIiIgcpt62uL5yVzfMmejn7jDqBGOtcTVJtmP+2TesnCpGCcYv2mOwTPuLf3ll\nFVLNVMY1liIUlJbjg/UntIrmXN+qokpi95ksPLUi2siehuc3xlgFZ9VZru9nquVcO1HWbi2vNmJ+\nOALmXO9evrmGeVU3qsfFRiddMUhaq5mrentZr/KxsajDrKx8a4q5IaY7EzIxf+tp3GdFF+LS8kpN\n4m2uNdVUa+yKfRd0umfbO1Z9fVyawWfHlhSW9wqJiIjIGvW2xbW6BWSOE4u1kPvpJ1ZlFVVo1NA5\n92ve+sN8Qr1crzuqdnfgryN01yVlF+MJ9fhQU2KTryLhsmErLAA89ZNuwmpLq/i1CsPW57Bj6fhN\nPTXMmYxCnLpk/U0LAEgyceNCwHxCbW7e4bDjphPPCL1pX6qTN1NdiGuazmjrycuY90+C2W30vfF7\nHDaqbzKczyrC0Yu56NulJQBg2e5zCLq5ldH9Ji7Zq2nZ15++Rpv2a5FSQgiBS3klmr9xSx7thw1x\nadh31pL5ek2ztXfK5K/26kzzxF4u5ChHL+ai6FoF7ujW1t2hEBGRE9XbFld9pqqakvvsP2ffF2xA\nVTVZ2zcR52wqGFPt4HnTMW00Ui1XW3SS7nQ1h1NMT19TVlmFvWezdZbp51jmPrHa+54y0sXYlJp+\nD1ZGJaO88vo2K/ZbXzm2JtrzqOo7kWb6tVRUmo49Ui/hO59VpJ7n1/j2Ib/E6jzXr0b83C+xmurK\n2pJzigwKTFX787BuS2WW1v5ztyRgyrIoo/tZOgZd+0ZI9Zy+j32ne/NDe4y1LbIKrtncbf9oah7y\nS6/fQNl3NtugJZzIFpOX7sNj3x9EuZ1TUBERkbK5rMVVCHELgFkAvKWUU1x1XlKmS7k1f2FdaCaB\nsdXGo/ZNtTF1+YGaNzJB/wu/udYzfT/sPW8wjnPl/iSdRG7VIVXxpvwS3dbVu7807KZsSny65Uku\nAPyuNyUOoCo6Ze1YYW2nbeh6DtR840Bft1lb8P2TQUbX5RYbjl89ejEXfh1boIGZqlTD50dgYt+O\nBsuf+yXG6Pal5ZUWzddqjP4NGO2bDsfT8hBxOkunhdPYa7LEVa34gj/ZYXQb/arOlvgq/CyWR57H\ngwM6mdxmeeR5vDHmNjRoUMMcP0SwfN5lIiKqnSxKXIUQP0I1F1ymlNJfa/l4AF8C8ADwvZRyrqlj\nqItAPCOEWGtfyM7B651r5RTVXPk0t9i2lh1ztKfaqU0+3WzYLTXitGHiW5282mpbvOlxopaydi5Q\nd7pkokuwsd7Kk5fuQ/cbm+HZoV2tPs/Wk4bv68aj6fhixxmcNHOz4P2/jptcZ46Uhl2nQ9fZdqzq\nrsb79XoAaKueUstaZZVVZj+zZZVV2HryMu7u08Gm4xMREVHdYWmL6wqoSuv/XL1ACOEBYCmAMQBS\nAUQLITZClcR+prf/0+rJzRVn++vDUFJeaXQqFXKeHacU+XEgB/jQRPEkiwjz0+y4iqmWm8TMQhSU\nmh8vvMnCll9LWoh/O5hi0bEA6MT1y4Fkw/XX7LsRZGvLsL2MjbUmIiKi+seixFVKGSmE8NVbPBDA\nWXVLKoQQqwFMllJ+BlXrrE2EECEAQgDgpptusvUwFuverjkAoE8nb6x74XY8+I3xcWZEVPdkGxmn\nCtTOHhh7Ek23iNrK2q7jRERERM5iT3GmTgC0+3ilqpcZJYRoo54AvZ8Q4j1T20kpl0spg6SUQT4+\nPnaEZx0hBAbc3Brje7d32TmJSJerG1u/3JlodHmaiallAOiMG61NbKkmfM9iy8dHO8vMNXFWtTxT\n/VUbbzgREZHlXFZVWEqZI6V8Xkp5q7pVVpGWTRvg7hCI6q380gqcybBvnlFn+5VJlMvZOtaXiIiI\n6g57Etc0AF20nndWL7ObEGKiEGJ5Xp5tBT+IiIiIiIio7rAncY0G0F0I0VUI0QjAIwA2OiIoKeUm\nKWWIt7e3Iw5HREREtdwPey/gjTWquYhHLoiAb2gYisvMF0sjIqK6w6LEVQixCkAUgB5CiFQhxDNS\nygoALwPYCuAUgN+llHaUEyUiIiIy7j9/x+PPI6qOXdVjzbXnsiYiorrN0qrCj5pYvhnAZodGBFVX\nYQATu3Xr5uhDExERkYL0bN8cCZcLbNpXsiITEVG94bLiTNZgV2EiIiIiIiKqpsjElYiIiOoHIWyf\nCIvtrURE9QcTVyIiIqqV2FOYiKj+UGTiqpTpcJ4YfFON23S/sZkLIiEiIiJzmMQSEdVtikxclTLG\ndc7E3prH/W9q6cZIiIiI6ibbOwoTEVF9osjEVWkaCODPF4egYQPDy6sdQ3OIiIjsJoT4UQiRKYQ4\nobWstRBiuxAiUf1/KxP7jhdCnBZCnBVChLouatuVV1ZpHh+6cEXz+MCFHFYZJiKqw5i4mmFJwYhb\n2rKrMBERudUKAOP1loUC2Cml7A5gp/q5DiGEB4ClAO4G4AfgUSGEn3NDNTS0e1urtg/8aJvm8Rc7\nzmgeP/VTNNbGpjosLiIiUhZFJq5KGeNanbbe6mM8OV0dMhjPDb/FdQERERHpkVJGAriit3gygJXq\nxysB3Gdk14EAzkopz0spywCsVu/nUkO6WZe4FpVVmlyXcqXY3nCIiEihFJm4KmWMa4MGAj8/PRCr\nQgYbXT/4ljZ2lfEnIiJyknZSykvqx5cBtDOyTScAF7Wep6qXGRBChAghYoQQMVlZWQ4NlJ17iYjI\nEg3dHYASbX51KHKLywAAw27zMbst01YiIlIyKaUUQtiVH0oplwNYDgBBQUHMNYmIyOUU2eLqbn4d\nW+AOC7su9enkjZBht2Bf6F1OjoqIiMhiGUKIDgCg/j/TyDZpALpoPe+sXkZERKQ4TFzt1KCBwPv3\n9EKnlk3cHQoREVG1jQD+pX78LwAbjGwTDaC7EKKrEKIRgEfU+7kUKwETEZElFJm4KqU4ExERkdIJ\nIVYBiALQQwiRKoR4BsBcAGOEEIkARqufQwjRUQixGQCklBUAXgawFcApAL9LKU+64zUQERHVRJFj\nXKWUmwBsCgoKmuHuWIiIiJRMSvmoiVWjjGybDuAereebAWx2UmhEREQOo8gW17qgg7eXu0MgIiJS\nvKJrpqe3sdaSXWdx14IITPpqL6qqrndBfvz7A/gj5qKZPYmISOmYuNqoTyfDqXpuaOShedzCy9OV\n4RAREdVKMcn6U9Da53x2EY6l5qG4/HpCvO9sDt5ee8yh5yEiItdi4mqF9upW1Mi3R2K1kbld7+93\nffq7d8b3cFlcREREREREdZkix7gq1R/P345DF67gpjY31LjtDY0a4lafpjiXVeSCyIiIiGonZxUV\nZrViIqK6RZEtrkqtKtzBuwkmB3aqeUMAEhIeDYSTIyIiIiIiIqr7FJm4Sik3SSlDvL0Nx5EqmXeT\n6+Na27XwQr8urQy2WffCHa4MiYiISNEE7/ESEZEF2FXYgV4d1R1tmjXGiB4+uNWnGWbd2wtr9KoY\nstowERGR87GjMBFR3aLIFtfaysvTA8/c2RW3+jQDoKosPKlvR51tOrZs4o7QiIiIiIiIai0mrk7G\nLlBERESudz6rCOtiU81uU1Ulsf9ctsWFnApKy3EsNdcR4RERkZXYVdjJut/YzGDZ66NvQ+dWTTDs\nNh/sSsjAu+uOuyEyIiIi93NW8d/7lu4DAIzo4WNym18OJGP2xpNY9sQAjPdvX+Mxn1kZg0MXriDx\nk7vh6cF7/0RErsTE1cleGNENwb6tMXX5Ac2y10Z31zz261C7ClARERHVJiXllSbXXchWTVmXnlti\n0bHiUlStrVWcaoeIyOWYuDqZRwOBQbe0wcKH++JcVqHB+o4tdYs19WzfHAmXC1wVHhERUb3HNJSI\nSPkU2c9FqfO42uOB/p3x9rieBsvbNGts0f5JcyegeWPeZyAiInIU1qEgIqo9FJm41tZ5XB1B8CpK\nRETkErb2+GVPYSIi11Nk4lqfMW0lIiJSKF6kiYjchomrwrS8wdPdIRAREdUL7ORERFR7cNCkgsyf\nEoDGnh7Yfy7H3aEQERHVG5bO40pERO7DxFVBHgrqgr+Ppbs7DCIiIpfx9HBus+ed/xdusMw3NMzo\ntj/tu4CPNsXj5jY3oP9NrRB6d09MXLIXq0MG4xafZuwpTETkRuwqTEREVAcJIXoIIeK0/uULIWbq\nbTNCCJGntc2Hro6zb5eWrj6lSR9tigcAJOcU468jadh0NB2ZBdfwvwMpbo6MiIjY4kpERFQHSSlP\nAwgEACGEB4A0AH8Z2XSPlPJeV8amTdSCdkzJmV6JiNyOLa4KY+4C/t49vWrc/+1xPRwZDhER1Q2j\nAJyTUia7OxB9Si6QxCnqiIiUg4lrLfLYoJsQ+fZINGxg+kKqX2DinfFMZImICI8AWGVi3R1CiGNC\niC1CiN7GNhBChAghYoQQMVlZWc6LUqFYu4mIyP2YuCrU3f7tjS6/qc0NWPp4f5P7eTfRnU5nWHcf\nh8ZFRES1ixCiEYBJAP4wsvowgJuklAEAlgBYb+wYUsrlUsogKWWQj0/9ua6wvZWISDkUmbgKISYK\nIZbn5eW5OxRFGte7Pfw6tDC6LrBLKyyfNgC3+jR1cVRERKRQdwM4LKXM0F8hpcyXUhaqH28G4CmE\naOvqAImIiGqiyMRVSrlJShni7e3t7lDcRntYzS/PDDRYP7FvR6P7SUiM7d0ejRt6OCs0IiKqXR6F\niW7CQoj2Qj2QUwgxEKrvBZxMXM3UEFd2HSYicj1FJq6ke1EcaqS77/PDb6lxPyIiqt+EEE0BjAHw\np9ay54UQz6ufTgFwQghxFMBiAI9I/WIJdcgdn+3EmmjDqW2kBKLOmc7Xo87loLisAtcqqoyuP325\nABn5pTiTUYDLeaU66y5eKcaF7CL7AneQlJxiJCkkFiVLzilCcg7fJ3KM6KQrKCmrdHcYdQKnw1EY\nSwsYstIhERHVREpZBKCN3rJlWo+/AvCVq+Nyl/S8Ury77rjB8hPpefhk8ymD5dVX2tMZBfD7cKvJ\n445bFKnzPGnuBM3jofPCDZa5y7D5yolFyYbPjwDA94nsl55bgoeWRWFi345Y8mg/d4dT67HFVQFa\neDnu/kH1bfKh3VVDlNo0a+SwYxMREdVFV4rKrNqe87oSkSUKr1UAABIu5bs5krqBLa4KEPXeKJRX\n6nY/srej1jvje2La7Tejg3cT+w7kIElzJ8A3NMzdYRAREVmMvZuIiJSDiasCNG18/cdg7yWyemiS\nRwOBzq1usPNoRERERERE7seuwnXETa2Vn6R+MKGXu0MgIiKyGBtcicgROLjAMZi4KszInjdiQkAH\n/Huin8X7BHT2xl09bwQAtG6q3DGtzw41XgmZiIjInawdnlN36y4TESkXE1eF8fL0wNLH+qNTy5rH\nps59oI/m8awJvRD26p24uU1Ti87Ts31zo8t/mh6MCQEdLAtWT/Ss0fBowNvTRERUu7DYEhE5E78d\nOwYT11qsZ4cWmseeHg3Qu6O3Tcfx69ACw25TzRUrBNC0kYdNx/Fp3hjnPr3Hpn2JiKh+qo1f6NiF\nmIiswVtjjsHEtQ6w9Po5sW9Ho8snBHRAHZ5vnoiIFEwJV599Z3OMLv9ww0mjyx/+Ngp/HUmFb2iY\n0Yr5WQXX0GfOVp11L/16GGm5JQj8eBsuZBdplm87eRnD54cbzC5gzCurjmDulgSD5R+sP473/zKc\nn9ZaUkqMWbgbG+LSbD7GC/+LxefbTiM8IRN3/t8uXKuotDsua3wXeR6Pf3/ApeesL1YfSsGkr/Za\nvd/4RZFYF5vqhIiUafHORDy7MhqAY2/MTfpqL1YfSnHgEWsfJq61mLXJpvbEx1ICb4y5DT/8Kwgv\njrhVs9xc6f+WN3hadJ7+N7XEwof76hyXiIiorjiRlo/X1xw1uX5XQgYKSit0loUdv4SNcenILS7H\n6ujrXz5nrT+B5JxiXLVgLtlNR9OxbPc5g+X/O5CC3w7a/4W2SgKJmYV4fU2czcfYcuIyluw6i9kb\nTyL1agku55XaHZc1Ptl8yuSNCLJP6J/HcSw1z+r9Ei4X4M0/TP++1DULt5/BjlOZDj/usdQ8hP5p\n/w2q2oyJay2mSVtt6LMkBPDqqO4Y1asdhBCaQhMCwLTBvsb3sfDYf744BA/076xJdGtDxWMiIqL6\njr2viEjJmLjWAbZ0QzB1bRIC6NPZG08P6WpknXVnqj7HuN7trA2vTvHv1KLmjYiIqM4zVgRKSbli\ndSjWXu/NHlNBr4+IajeXJa5CiPuEEN8JIdYIIca66rx1mTMvBuP929t9DC9PVZGnJp7Xiz316WRb\nASkiIqK6Qhi75ayggk+OCIUFrIjI0SxKXIUQPwohMoUQJ/SWjxdCnBZCnBVChJo7hpRyvZRyBoDn\nAUy1PWTSZ83FoYO3FwAgZJjunKr6d4EHdm1tsO9ro7pbFdejA2/CzNHd8eLIbpplm16506pj1HYP\n9OvEu81ERGbUxfzGaGJaCzjjesVLIBE5iqUtrisAjNdeIITwALAUwN0A/AA8KoTwE0L0EUL8rffv\nRq1dP1DvR3az/nLQ6oZGAIAeevO4Xh/javxi++UjgfjXHb5WnatRwwaYOfo2TctrfeTTorG7QyAi\nIkVTXmrniNbS2pm6E5GSWZS4SikjAVzRWzwQwFkp5XkpZRmA1QAmSymPSynv1fuXKVT+D8AWKeVh\nU+cSQoQIIWKEEDFZWVm2vq56QbugkqVquhhpr7+r540mt3NExeB/Zg61aLsvpva1+1x03af398HK\npwe6OwwiojrL2FhWwHyLphJaaU3Fbdcx2e2IiL8HDmLPGNdOAC5qPU9VLzPlFQCjAUwRQjxvaiMp\n5XIpZZCUMsjHx8eO8OoPRxRRMPb79OP0YJPbvzO+p93n7NnesqJFt7RtZve5zOndsQVaN23k1HM4\n09HZ1g0Z73ZjMwwy0hWciIgc4z0TU1b8EpUMAFi2+xzSckvw7tpjyC68Pg1OVZXE7jNZ2H0mCxWV\nVZj3TwL+83c88kvLDY5VWl6JTUfTcVxrepKLV4pxNrNQ8/xSXgn2nc3GsdRcnMsq1Nk/MaMAF68U\n6yzT730VdzG3xml6LmQXIUlrXtpq9n43SckpNohZStX7U1mlCjS78BqOpebWeKyYpCsoKC1HaXkl\nos5dnyonUv0+A8DlvFLEp+cb7CulRMTpTFRVOS/xiFbH50jJOUU4r/f+AUB5ZRX2JDq/Yaj6Z6X9\nvh08z2mK3MXU72lt09BVJ5JSLgaw2FXnqw/8OrbArT5N8f491ieR+olq9V1W99/vNeSoAg8Dbm6F\n2OSrBsufubMr7g3oiNs+2OKYE2lxxR107yaWza9bTUpZ77pvd2ndBBevlLg7DCIyoi62Q5jKcS7n\nX5/TdMjcXTrrhABWRiXho03xAIBOLZsgLVf1dys+PR+rQgbrbD9n40msjr6os2zovHAAQNLcCQCA\n2z/TPYe2MV9E6myrG4zqv/uW7kP3G5th+xvDTR5n5III08exw7D5uq8FAHacysSMn2Pw3t098dzw\nWzFxyV5cyis1e+7CaxWYsiwKQ7q1QbsWXvjzcBrC3xqBtKslePLHQ5g5ujtmjr4Ngz/bafR1bDya\njtdWx+E/k3tj2u2+Dn2NAJBXUo6HlkVhaPe2+OWZQQ477vD5EQAMX8/C7WfwTcQ5/P7c7UbrmTjK\nPycu44VfD+Pf9/pplk1dfgBR792FDt5NnHZepXJkpW5bOOv31NXsaXFNA9BF63ln9TK7CSEmCiGW\n5+VZP8lxXfLBhF5YrXeh0nZDo4bY+eYIDLjZ8j88pn5vqhOZBg2Ul7o6u3fFkG5tnXr82tg7pH0L\nL3eHYJcbGukm5r8/d7ubIqk7ut/o3J4PRASdG2zVSSsAnEw3/D50JqPAJTElZhq22lnDkZfA6sQ/\nRd1SfCmv1NzmAIDyClWL6sn0fM17VlhagcwC1b7JOcUm9wWA9FzVdqm5zrn5WaaO79Qlw9ZeZ7iQ\npWp1yym85tTzpKt/Nvqt+sVllU49r1Kxq7Bj2JO4RgPoLoToKoRoBOARABsdEZSUcpOUMsTbu35P\nnYZddpwAACAASURBVPLs0Fsw+JY2DjlWQGfz7+W8KQF45a5uGOhrPAnu1UHVrfeXZwZi3pQAzfIR\nPRzbnfuR4C41b6SnuZdlHQeM/dHo1LIJ2jkxSaut0wFYOs5p2RP9nRyJbWZN6KXzvFljl3UuqbPs\n+Sz37si5jN1FCJEkhDguhIgTQsQYWS+EEIvVswMcE0Io85e6nnPVV15bameY4szLn73vh4SslTeV\na6Pa+j3IUer763c0S6fDWQUgCkAPIUSqEOIZKWUFgJcBbAVwCsDvUsqTzguVbJXwn/FY98IdAIC7\n/TsAANp561a7vbG5F94c28Nki+tt7VRViId298HDQarkMvGTuzH3gQCj2ytJl9aqLinGrlHhb40w\nuk8P9et1RMLjimujh5tayju2VGZ3H08P03/a3DWXcBOFdM/272RbEmnPl7zvngyyfWdyhJFSykAp\npbEfxN0Auqv/hQD4xqWRQZlDVEiZ7P2saA/fkZplyuCqRNoZBbiMnod3BsgJLK0q/KiUsoOU0lNK\n2VlK+YN6+WYp5W1SylullJ84Kih2FXYsL08PzRf5F0fciuNzxuLG5va3Mnp6NICj8yVjd6Zq/NNX\nwwbmxpk2amj8V6BPZ2+c/eRuDLvNud2I7XVbO+d033T09ebB/p0de8AaNDdzw2HDS0MsPs5/7/N3\nRDgAUGMlZ2eONdI2Ncj6Xg3mPDf8lhq3seYGR0fv2t1NvRaaDOBnqXIAQEshRAd3B0V6jPxNrs9p\ngS3XKO19lJRT1fUWOSVUy3YnJX3W6gJ7ugo7DbsKO48QAs29LC/m838P9sH9/cwVi3av/0zubfG2\n+n883h7Xw+S2AkBDjwYIGWbftD/O/nM9OVC5PxttrW6wroCUvcb7t9d5rl0UwZJx3O1aNIZP88Z4\nYvDNdsXh0/x6z4aaurS3sLDLe23TtJHlLc1d2zbFcCPDD6wtQEY6JIAdQohYIUSIkfUWzRDAqerc\ny2VdhasLNTrw4uXIL+62xKW9j5KTRFf9jOt7Ikm1myITV1KOqcE34YupgU47/qx7ehksi/twjKar\n7s1tbjC7f3DX1gZJir6p6nGz+lPevDSyW43xBXZpWeM22l4ffZvOc0deiEb3Mj2vrrtYegF09Q1H\nIQTG9W5ncn2/m8z/XA+8NwqH3h9ldxzWVfx23JcJpYwpbdO0ETa/Zny+Zr8OpmI0fB/a1OLpqhTg\nTillIFRdgl8SQgyz5SCcqs75zP0FMNbt0pktOQ5JbpyaH9n34nX2tjROJ73frk4jXdVVmFSUfLOk\nNmLiSg6h3bJkibbNVNvPGHYLTnw0DrdrilAJtLyhEbw8VR/NRRYkzZ8+0AfRs0abXP/iiFtx9pO7\ndVq83hhzm8nt7dG6qWUtQ9otSL89W3P5++l3+OL7fxnOq2vLH0R3XLLc0VXm22nXh/Ppf+l7aYTh\nTYs/nr9eeVgI4ZDS9R4Nrv+JdeXFy96pBqYGdcHLejd2bPkRTg7shJvbNDW67rcZhp97Xt8dT0qZ\npv4/E8BfAPT7rDtthgByHCdOIapD6d0a7U2otfeu72MwnX1N0hT60jsP/86TPRSZuHKMa9125N9j\nsPvtEZrnzRo3xL199YZUaf2lM3dxERDw9GgAn+aNMaSb8QrMQgg09Gigc0G+N8D+IVxBN7fSPH6g\nv/EuuwLm42/YQFhUOdpY90nAeV8yPpzoV/NGdvL0MH/5CvZtZXa9Iy17YgCCTVTUrja0u/Xjne+u\noTeAI2kn3vaaMayrQffcUT1d0OJv4iMxxs906zmZJoRoKoRoXv0YwFgAJ/Q22wjgSXV14cEA8qSU\nl1wZZ32bV9qUAf/dgR/3XTC6rqS8Em+sidM89w0NQ9zFXJPH8g0Nw6U809O3+IaG6Tyu/ndNPTWL\nEMCmo+kG23y+7bTOcX6OStI8fmR5FJ5deb1w9Xn1tCuAxPD54fANDUPwJzvw7Mpo/N8/CQCAT8Li\n8fwvsXjulxh8EhaPt/44itB1x3TOMfXbKCwNP6uzzNS175HlUZrH0344iEU7zmie55WU42iq6nvl\nfUv34e21qvP8eThN5/0w5dvI8zibeX0KotdWH4FvaBhikq4gOacIvqFheO/P67F/tOkkXvrtsMFx\nXlml2i+3uEzvNUk8/v0BzN+agIGf7MDOUxl4ekU05m9VvVc74jMw+NOduFZxfTqZ8NOZ8A0Nw8fq\nuX+PpFxFwJytuFqke2xTXlt9BP9er/8nwbjisgrN45mrj+DDDYb7TVi8B79EJeGpnw5hgfqz8sNe\n3c/0+C/3wDc0DLe8d/09z8wvhf/srYhPNz4lUMjPMfANDcNGrc9k4bUKDPjPduw/m62zbWZ+KXxD\nw3D/1/sMjnMyPQ++oWG4bdYWlFeqPuuRZ7IQ9N/tOq9PW1WVxLB54ZrfgcJrFUi9Wgzf0DB8uSNR\ns93cLQk6n39A9XuzMU4V87msIiRlq34nPt92Gv/68RCe+yUGn24+ZXDO7yLPG42/2sJtpzHwkx3w\nDQ3DB+uPAwCGztsF39AwBP13u2a78IRMze9dtdELdwMAXvhfLGauPgL/2VtxJOUqbv9sJ7advKx5\nvRvilHn/UpGJK8e41m2tmjZCU73iOaYuQjXlZdpdXr57MggbX7as8E5NrWk13Ykc2cNHp9iOPRVj\nHZl7vjaqe43bmHtpjRo2QNLcCbg3oKNF56tuGbdFTa2CLawYi12Tmn7eA26uOUnuaEMrprnqxo50\n4qNxNSbe1uh2Y3PN44eDOiPqvbvg56Lux//f3n2HSVXdDRz/npntHdill116U0AQQYogvoAgYAyJ\nYNQoKrG90RhrTGJ5Y4L6JraoqIktRXxV7Nh7CaIggoAoZUF6kbLLLtvmvH/cO7N3ZqfuTrkz+/s8\nzz5758ydO+fce2fO/O459xx/h+raqZF0uRYWHYCPlVJfAcuAV7XWryulLlZKXWyuswTYBGwAHgEu\njXcmT+pbEvVBw1LR4i8j+yH5+te7In6PHZa5Sn0DSID73vUOIH//YuNkEks3/cDb63b73a57rtS9\nFTW8vW4PD76/EYBHPtrM62t28caa3Tzy0WaeXb6NRZ9/7/Xazzb/wJ1vGEGQ+/sh0G+GpZt+8Cx/\n9N0+7rYEFtGwaFlj3l40A5L73t3A8+axecry/GOflPPqqqbXgNwXBFZsPQB410+fbNjP/e9tZE9F\nDbe9uo53v9nD/e8Z++rml9ew6/BRdh9qnH/1D68YAav7gsf9723k8NF6Pi9v3A/BvLhyB/9YuiWs\ndTfuOeJZfmHlDp78T9PXrdlxmN+9uIb31u/1XATx5Z631tqL4J1v9lBZU+91IcTqzbXGeXWD5Zxc\nt/Mw+4/U8ue3vvVa9+11ewD4cmvTCzvuILq2wcXBqjrACDj3VdZaLrT45LfB5Zk3GGD9rgqeNs/R\nuywXRhZ+sNHv+X+v5TOzeMU2wDhnPvh2L2+s2c3DH25q8prblqzzm3/rNvdUGOfBP5duBRrngN5X\n2XjR4g+vGufH3orGc2aDOS/za1/v4oWVO6isqWfBa9+w89BRbnl5rae81z7b9PNvB7YMXEXr5f7+\ndn+NR9KimJORRu/2gUfZjWaA2KNdrlfwHSguCpTunr7m9DAHvhrS1f89mY2VeNPS3TLT/8BVLd0P\nZcW5lORncs+cofTpkB/6BRbWADHSbkq/Py16rcDW937zV+Mj7ureUo+dd3zURs8tzssMe9qmh88Z\nHnKd//3JEKDxolBBVnqzux5HK9hN1HRPyU5rvUlrPcT8G+Qe/V9rvVBrvdBc1lrry8zZAY7RWjeZ\n6zXWHA7F7bPtP7VaaxDNeVx9t5mqYlU83+166nvLM4HeOxZ5Sol7Na0jS7dgLzW3TrLrR8Ffvuya\nVwlchS1ZvyAjGQU5HB0LsigNMehTrDkUfHXTZBaccUxY67sHllpzy5Sg61n3W4eCyAOjzDBaCGcO\n6cznN54SdETj/h29A1p3vjIDTD8UyK2WUaPnjS2L6LXhKgoxYu1lE3t55kGOFodDBZyKKZBHz/M/\nF6r1mH9y/clB7/cuLfZ/v6nV7OHeUxf5/liZMaQzb181nksnBB9x+61fjefHAbrQg9HNf4jP4Gep\n8LtIiFQQjXv8WwutdbOCc9/7dUNtwmEeE6/38nlRqBZpuwuVb+t5GegMDTsgDXAPbjgvTGtu4Jqk\nx8VObBm4yj2uwj1YU/v8THq3z+Px85sOTBQpd6vkDdP6h6yUh3aL7P7K5nwZFWankxbhXLi+Xax9\nWSvCSc0YhfjRKOxngJtmDPKaRqm5X9bnji6NSn58RVJRXTOlv1dLcbRGuHXvEn/T4MwY0rSrdjj7\nsEtRdtDWYwVhFz7Q+ymMrsR57nwHWK9Ph3z58StEkrH+6I/W5zeav9VT6hslRGF8e1K5V3cFHffD\n8+rm5ioh7HBcA9Z5fjIXzrR6Sc+mp5AtA1e5x7X18f18/HpyPz68ZiLd2hotoxP6tef0oU1/zDen\nNTZYZexQ8ME1E5g70v/9VheNC9Hq57PtcEZAVEpxc5QGQ2pjGdW4OfdX9mgbuiU61G+ZKYM6MKK0\njXfg6ucbMNSeifQ30//9IvDgRBnB9kWE73N1kPl/g+mQ778FfPGlY/j2D6d6pY0NMNBYS/UqCdyV\nPhDfz4v7SLZ0dE+/54SSGQaFSKRYdBWOhdY4pUs4FxLCaXFt7oXkeFyHjMZxjbR8kZZLa3A2c2ck\nw3lr9+vNtgxcRfJwd3fsVRK6C2I43J8Xp0PR3ac7791zhvHxdRM9j5+/9ES6FIV/7124Xxc92uUG\nrCA6R/B+gUw/tpPXVC0AAztH5yLNnOO7t+j14QS7oUZ4feicEaQ7Hd6TvsfhZ9DIssCDE/l2yz2m\nS2Tz84ZjRIgBntrkZlimffI+18PpNhyNLkaRXCWO9O1yMiIboKwwO71JoWxeXwqRVFKxW6Jdf1S3\nZFcH7vIa+XvFsq6N6bZtMLVfJEGlo7mBaxJ9Ju0aZEvgKlqkKCeDJ+eN5KGz/d9/F21d2zQGs8O6\nNw0U0hyhT+mWfPWG86Uz09LN09932/1nHddkFNuRZW1ZduOkZufLvb3mDhjQv2M+T8wbSZswusEO\nilKQ7cvfvJ6xUpKfSXFeZF1+/R3Lx847nllmT4CxfYpDjmqd5mcKIH8Dax0bYDCucM0bUxrwuXCn\nGQrV8nLWCd2ZMqgD88f35LfTB/DiZeGN6O2eZsf34tCY3u24ffaxzWoVFkJEhz1/qjYVSQBg134c\nkebKX2tqc45Xc4P/uLS4Nute4Wa+V5xf19LXxoNdLwxZhTcUpRBBjO/rf47RSLi7qPbrGNkotb4y\n0hw8et4I5j3evIExm3tPT5k56E2ngizuPnMope1yPEOgh/tF3D5AV9Jw/P2849myr6pJ4OpQ4U1c\nX5CdzkkBjuOMIZ0pbZfTZBoEt9+fNpBbX1nLhH4lvL9+b9h59t3XPdqF32o/oFMB63b6n+8tUuH+\nqBnVsx2LPv/e6xyd2L89X1rmUowk4PQt/3E9ijxD6Q/oVMC3fziVvr99ze9r7zpzCL96+quA2z6x\ndzHlC6b7nZuwf8fGkX6L8zK8hs73n1H/yYXZ6Z6eAxeO6xl8G8Conm1ZuukHXv3lWL7e3vTY/evC\nUQAM7VrEreYUD0KI5mvO52jFFmOKlooa/3NaAizb/AMPvr+BuobwKrfHPin3m35PkKlqJt/1QZO0\nq55e6ZkS6Jnl25pM+bLczLuvBeacseFyT1MDNJkr9wvzPazrrN1xmI++a5xL1OXSTebmrKqt5553\nvvO6sP3M8m08s3wb883vT/f0LG7WqYlWbTvoma5Fa80/l27hwJFaNu/znsJld8VRYx1L2qMfb2ZC\nvxJ6mhcFv/reGD/muRWN0yu9t34PPYtzOVBVx9odhzlQVUtdg4uzRnZn1bZDKAWHj3rnD2B/ZQ2r\nth1ib0UNh6qbPh/Kr55eyUXjenqmqTlSW89/Nu6nTW46z36xjZ+M6OaZbxWMuVuXbzlA+/xM/v2Z\nMQ3MF1sOUFlTT26Gk3e/2eNV9kXLtjJpQAdK8jPZsKfSa0qpVdsO8fX2rV710Wurd5KTmeb5PbRs\nc9NphV5bvYtOllkBPtu0n5yMxnDq290VTV7jVt/g4qxHljZJf2rZVob3aMO+yhpuskwxddp9HzFz\nSGdGlLbltdWBp9e2zlcMcKiqjsKcwLfSfbnV/2dl24FqnvzUmOaorkHz4PsbmT28Kw++v5HN+yq5\nZEJvKmuM4zyhb/uE3Otry8BVKTUDmNG7d+9EZ0XEyfi+Jbx8+VgGd2n59BmlAQKg66b053B1HSf3\nDzxokb/Wr3DMG1PGgE4FjOldDDROA9KnQz5vrvE/r124+oUx5UxBVjrHdG3aEvrGleO5bck6r4Ay\n0P4J5L65w4z/AQLXeWPLmDe2rMnIitapiaJ9FW/BGcdwpKaes/72WXQ3HMTpw7owtk8xxXnG4Ee5\nIbrGBroQEMjF43txx+vrPY99uw9bz8zRPYs9yy3Ztc9cfCIT//d9v8/lm4Mv+c6n29z3e2LeSKpr\nGyjKyfDqOeHL4VB0bZPNtgPVAdcRQsRGOMHuTx/6T0TbfGrZVr/pd/n82Lb6dndlkzTfeWzL91d5\nPf7xg59G9P6BnPFA43ZOv/8TrrPMI73y+4M0uLTXOvuPeF/8u+mlNU3mRh1/x/vsq6zhoQ8a5+x0\nz/Hqb65XwOvCwMy/fuJZPlLbwG9f+Nrva9xzf1rr4ltfWcutr0D5gukA7DpsBLdvrW38bXL+Y5/7\n3V6oeXB/9MCnXnOcRur5L7d75sAFWLJ6F0tWN84//DczoLXyd5yveOpLThnYgRsWr/Yarf76xatp\nn/8ty248hVP+4n0x5KInvRs49lbUcMm/VgDGvjpSU+/3XH/0k83cOG2A5/GZD3sHopPv+tBvWcGY\nt9ifGxav9pv+9fbDfi/0+vI9TvOe+DzobAg/esBnH1rOl9ssF11uf/0bbrdc+HnP8lvy1lmDYjaA\nZjC2DFy11i8DL48YMeKiROdFxI+/wKs5AoWe3dvl8I8L/HdHDRZYHdu1kFXbgo9w7XAoT9AKMHVw\nJ167YhwDOhWwdsdh1ge5AufPLyf14YH3NvDe1RPC6r5r9dG1E8lMN4KePh3ymXN8d6/AtaOf+UP7\ndgive+Z5J5YGfE4p5bUfQ8396bvLI7lokJvpZEi3InIznBypbQj7df5Ech+HO2h9/cpxtMv1P3pv\nx4Isdh0+Stc24d0P7X73RFy5LCvOZc7x3VhkTqZuNXdkd7TWzBnZsvum3TLTnGSmeQf7du82JYQQ\nkfLXG2hfZU3Utm9tgUy0lgSt0fTNrgpPg8HOg94XPfdUhLfvq3x+S9SH2aPAjtbuiKxHWnPuZ91+\nMDEXl+UeV5GUJocYIKi5/HUVfunysZ5ldzfRId1CB9kDOhlfotdM6RfxwDVX/VdfNvxxGt3a5pAX\nYgocX93a5kTU7fipi0bxu9NCj2pcvmA6N88cFHI9f/zFZC2pEnq3b1mX8pbq37GgcdoZn4D7spPD\n6ykSaZgaKLCP1T0pTofinNGlEY1OPdPPND5CCJFKmtszK3rvH8Y6reyyoEtrT50ajXtXtdZJPVpg\nKh9/CVxF0vnsN5O476xhYa177dTmTV0SyJjexXx4zUR+NKxr2K9xOhQn9ioOvWKMuAObLkXZ/O3c\npoNoje7VrklLWLSMNVuhmzsCXyiR3pOc6Wf03mgN3BHOdk4ZYFxwKbWMmB3ub6DsCC9+hCvYaMxW\n4eTz3rnhfS6FEELETjKNXhsNXgNWRansyTBQUWskgatIOh0KssIKtHoW53LphOjfJ+07TU847p4z\nNOr5iNTAzgWcEqOW6kDcX/zuLjyXTOjV+FxccwJ/OH0wr/5yXNzez1/dee7oHnx102RjIKoId8DY\n3sWMLGuLUtA2N4NxfaJzMeSM48K/CAOhK/PyBdM991IJIUSqCRUXxTrgCafFt5XFrUYLo7njm9si\nbn1dkje4pvSFCwlchbCI1RdVpN19oynQF9g1U/p5BuDx5/6zjuOBnx0XlTwU5WRQvmA64/qUcPXk\nvrwQ5tQpbqN6+m8VvPtM44LAnbOP9aT9ZLj/QOzsUT28BoyK1Rd7sHNIKeWZDqZReBlRSvF/vxjN\n5j9NJyPNwR1mmaPRYhzuFDnRFugYRCsoF0KIVJPCMUmzuSyBZjT2j+xj+7Ll4EwyqrAQ4YkkaPFd\n87KJvblsYuDP2PRjOzUzV8FdfnIfv+kdCrIY1r2IIV2LePzTck/6mlumNBlh1+2UgR28Wvea09KX\nqO5Adrqae9eZQxl7+3u0CTJ8fjzdMnMwl5zUmwNVtXz0XfhTLAkhhGh9on0hWmvd7OkR7SDS3ZFM\nLbS2bHHVWr+stZ5fWBidUWaFCKW5H9r8zDROGRB4ep1YWHzpifRv4Xy3tuBTJ6Q7HTx/6Riv0ZkB\ncjPTIhogKBX5a6GOZkXjnoMu2CaH9zBaZWcHaNFuiYVnD/d6nJHmoHu7HIZ0Kwp4oUMIIVqjsAZn\nSqZIJAq8u/k2t6uwZRl7XVyOtWSK0W3Z4ipEokT64V19y5TYZCSI47q34aR+JXyzK7IpdoS3aFfr\nvqP4hao7zxnVg5tfXktJkBGg/3nBCTRozVDLvHS+olHhhLOJbm1zYnbvauei8EfBFsJtwJ5N7M8u\nZE9+u0RnRSRY6fWvxmzb1nksAfrc+FrQ9T8vP+D1ONp5m/vIUr/p1ve5YtHKoM+nmv1HarnnHWMu\n0wNVdU2eD6fsVz7duM8G/v51r3l0fVnnOrWj2npXRMfb95wNx0MfbGLXoaPcMye+gzJK4CpEnMwa\n2pm+HVKgpTTGgl0tfeGyMbSLcF7bUFoc9/lEjuEGkueNKeO8MWVB1xmb4vd6pvKQ/aL5XrhsDKff\n/0nI9Ra8dh99923l8eEzeHDUbA5nhTcftRBCBBMsaBWNXly5I+6Ba+vufyeEKR4/oO+ZMyzoPaWt\nTej4rukaQ7sV0a1t5KM6p6Jcc8Cv8X1KWrytzHSjKhjdMzEtV9GakkikhmA9DKwun3Udr/cdzS8+\ne46PFl7AxUufJavuaIxzJ4QQIlEkcBXCIll+QE/oa9xXG2i0XZGcTh/aOezgsTA7nY+uncj/nD44\nrHUvGBu4dTcnI423rzqJu86M77RN7tG2HVITiWb4vqgjv5pxNdPPv4cvug7k+g8e54OH5/OzL5eQ\n1lCf6OwJIYSIMukqLEQY/nHByIROaeNrdK92bP7TtKQe9S7R7px9LHe+sd7P9DQtc2wXo7VofDO6\n+d4dYZebcFufv7ppcsh1rFMFxcu9c4fx3PLtDOxUEPf3bg2UUt2AJ4EOGLd1P6y1vsdnnQnAi8Bm\nM2mx1vrWeOazpda178kFs2/i+O+/5roPnuC2Nx/gws+f58/jzuHV/mPRSq6MCCFEKrDPL3ELmQ5H\nxFuogXTGRaE7ZrSFG7RmmlPJ2CnwhvDzHyuTBnRg0oAOUd/uMV0LWXvrFM9IvSKw9vlZXDKhV6Kz\nkcrqgV9rrVcopfKB5Uqpt7TWa33W+0hrfVoC8hdVn3cbzOyf3cGkjcu45oMn+etLd3DxZ89xx/hz\n+bDsuOQaOlMIIUQTtrwMKdPhiIRJwd81J/Ut4bqp/bl51qC4v/clE3rhUHBsl8R8lheePZz75sZ3\n4ABAglZhC1rrnVrrFeZyBbAO6JLYXMWYUrzT+wSmnX8vV572awqPVvLkMzfx1KLfMGz7N6FfL4QQ\nwrbk15UQKc7hUAlr1TqxVzGb/hSbKVTCMXVwx4jWf+ic4eRnydeiSD1KqVJgGPCZn6dPVEqtArYD\nV2ut18QxazHhcjh5YdBElvQby9yvXue/P13E8/+8mjf6jOLO8eeyobh7orMohBAiQvILTaSckvxM\nAM46QX6Y2JkdG7enDIos0LW7ly8fS2WNDFLT2iml8oDngCu11od9nl4BdNdaVyqlpgEvAH38bGM+\nMB+ge/fk+W6tTUvnieEzeHbwJOZ98SLzly3mjUcvZ/Ggk7l77FlsL2yf6CwKIYQIky27CgvREgVZ\n6ZQvmM6F43pG/Fo7BlOtzciythRmp3PpRLn3saWO6VrI6F6JmeJG2INSKh0jaP2X1nqx7/Na68Na\n60pzeQmQrpRqMrKY1vphrfUIrfWIkpLY3PM/1XLhaGCnAroUZUdt20cyc7hvzFzG/+JvPDpiJjPX\nvc+7j8znd+88wqBdG8isr43aewkhhIgNaXEVQiTEryf347J/r6DB5T0yVlFORlij4AohglPGCGh/\nB9Zprf8SYJ2OwG6ttVZKjcS4oL0/jtn0WHjOcM/ykivGAVB6/asAlC+Y7lluiQM5hdx28oU8NmIm\nV3z8FOctf5kLvniRBuWgvE1nvinpwfqSUtYX92B9SQ+2FnXE5XC2+H2FEEK0nASuQgBpDqOtde7I\nxHaBO39MKRP6tY6ua1MHd2TjH6dF5cdoIp3Yqx33vvNd2POvChFHY4BzgNVKqZVm2m+A7gBa64XA\nbOASpVQ9UA3M0TrUOOvJb0dBe66bdgX3jpnLkJ3f0m9vOf33ljNwzyZOXf8pDoxdUJ2WybfF3Vlf\n0sMMZktZX1LK3twiGaVYCCHiTAJXIYA0p4Nv/mcqGc7E9p6/aUb8R/4VLTOqZzu+u+1U0hN87gjh\nS2v9MSHugNBa/xX4a3xyZD/bC9uzvbA9S/qP9aRl1x6lz/6tZjC7hb57tzBx0xf8dPXbnnX2ZxeY\nQawR0G5q15Xyok7syWsrAa0QQsSIBK5CmLLSpTuYaB4JWoVIHdUZWazq1JdVnfp6pbetOuRpme23\ndwv99m7hp6veIrfuqGedqvRMthR1YnObzmxp09n8bzyWoFYIIVpGAlchhIih3AwnR2obEp0NIUQL\n/ZBTyH96DOE/PYZ40pR20eXQHsoO7KDHgZ3m/x3027eVUzYsI8PVOKq3O6gtb9OZcglqhRAiLSse\n3QAADlJJREFUYhK4CiFEDL151Uls2luZ6GwIIWJAKwfbijqyragjH5V5P+dwNdD58N4mQW3ffVuZ\n5BPU1jjTqMzI4UhGNkcysqnIdC/nUJmRTaVl+UhmNpXu5YxsKjMbl6vSs6hOz0Qr6QUihEg9tgxc\nlVIzgBm9e/dOdFaEEKJFuhRlR3VaDyFEcnA5nCGD2tIDOyk9sIMuh/eSV1tFbm01+TXG/7ZVh+l+\ncBd5tdXk1laTV1sd9nsfSc+iKj2Lqgzzf3oWRzKyqU7PbPyfnt34fEY2R9KzqMjM5WB2HgeyCjiU\nncehrDzqnOlR3jNCCNE8tgxctdYvAy+PGDHiokTnRQghhBAimqxB7cdlw8J6jdIucmuPkltbRV5t\nNXlmgOsObHNrq8mpO0pu7VFy6ozlnNqjnv+5tdWUHDlAbm012XU15NZVk1NXE/J9KzKyOZSVz8Hs\nfA5m5XEwu8D8b32cz8HsPA5l5lGTlkG900mdI416h5N6Zxp1Dif15uOEdYnWmjRXA07twuFykaYb\ncLhcOLULp8uF0/2cblxGg0O7cKBRWuPQGqVdKMx0baQrrXHg8qzj0BrQuJTD2A9Oo/x1PvujweGk\nzue5BuWQbuNCBGDLwFUIIYQQ4sGfHUe/jvleaReNK6Nb2xw+/HYv5fur2LDHuyt+WXEum/cdiWc2\n40Irh9EtODOH3VHaptIusutqzIDXaO0tqq6gTfVhCo9WUnS0gjbVFRQdraCw2ljuVLGZwqMVFFVX\nkKZdEb+nO2hzB2r1DqdXcFfvcKDMgFGhjf+6MVAEd3DYGEB6gkd3utY4tYs0V4MZiLo8Uxwlg9og\nwW69w0mdud/qHWmewNea7hssa6XQSuFSDjQKl/nYvexSDss67nQHLqUAhdIuMhrqyGioN/8bf+nm\n48z6OtJd9WTUe6dbX6OiMMuWSzmodzhxORw0KAcNDqf530GDctLgcDSu4053r6McuBxG+VtCK8xt\nOak3/zcoB/UOBy4zD9b8uPNpzZe5Jct5rL0vgrgvilgvmKC9znU7qMrIBqbH9T0lcBVCCCGELZ16\nTKcmaTdOHwjAuaNLAZrMBf3e1RM8aX3a5/GdJbB97pLR/PKplWw/GH6321SmlYOqjGyqMrLZl9sm\nwhdr8murKKyuoOhopSfYzWioI62hgXRXPWmuBtIa6kl3NZDmqie9wfif5mogvcF83pKe7mrA4Wow\ngyiHJYgy/lAKFw5P4OUdgDm8HvsGLQ2OxsCmweGgXvkPgFyWAMgT1EGT4M6d7ptPrRxoBRqFQ7s8\n5Ux31ZNmKWeap/xN09zr+tuH1nTrPsyuq/G/b10NRrDj1WpsCYJobCU21nFZ1tE40LhQ1KalU+tM\np9aZZv5Pp86yXOtMoyo9i4PZ+T7pxnM6Cq3IDp/Wcae7ddxltJRbL1R4lrWLNFc9mWbreku5L4o4\nze2nuVw4dIOZLxdO3eDViu+7jvtij8tyAcFz3ljOdY3lYoPPuW78T7xDWXlxf08JXIUQQgjRSihc\nNmmtSHpKUZGZS0VmLtsSnRcRW1pL9+VoSbF9WR7n95Nh54QQQgjRakjgKkSEUijQSjjZly0igasQ\nQgghWg2JW4UQIjlJ4CqEEEKIVsMlgasQQiQlCVyFEAn189E9Ep0FIUQroqXJVQghkpIMziSESKhb\nZg3mllmDE52NVmXR/FFsPyCjqorWScJWIYRITtLiKoQQrcyonu348fCuic6GEBE5sVc72uVmNEm/\nfGLvJmldirIB+PmJpV7pSsHVk/vFJH9CCCFiSwJXIYQQQtjevy8axfLf/VeT9Kun9KN8wXTPH8An\n159M+YLpnD2qhyfN7awTujN5YAcAFp493JP+xx8dA8Dckd1Y8stxXq/580+GRLUsQgghIieBqxBC\nCCFavcZpcpTMWCGEEDYkgasQQgghWj1P2CpBqxBC2JIErkIIIYQQZourQwJXIYSwJQlchRBCCNHq\nued3VdJVWAghbClugatSaoBSaqFS6lml1CXxel8hhBCiNVJKTVVKrVdKbVBKXe/neaWUutd8fpVS\n6rhE5DOegsWj7vldlTKCVyGEEPYSVuCqlHpUKbVHKfW1T3rQStFKa71Oa30x8FNgTPOzLIQQQohg\nlFJO4H7gVGAgMFcpNdBntVOBPubffODBuGbSZhqHZhJCCGFH4ba4Pg5MtSYEqhSVUscopV7x+Wtv\nvmYm8CqwJGolEEIIIYSvkcAGrfUmrXUtsAiY5bPOLOBJbVgKFCmlOsU7o/GUkWb87GlrzgebneH0\nPJeXmQZAYU4GmWneP49yzeeEEEIkjtKe4d9DrKhUKfCK1nqw+Xg0cLPWeor5+AYArfWfwtjWq1rr\n6QGem49x5RegH7A+rAwGVwzsi8J27CYVy5WKZQIpVzJJxTKBlMuth9a6JFaZsQul1Gxgqtb6QvPx\nOcAJWuvLLeu8AizQWn9sPn4HuE5r/YWf7UndHL5ULFcqlgmkXMkkFcsEUi63sOrmllxC7AJ8b3m8\nDTgh0MpKqQnAGUAmQVpctdYPAw+3IF/+3vsLrfWIaG7TDlKxXKlYJpByJZNULBNIuUTLSN0cvlQs\nVyqWCaRcySQVywRSrkjFre+L1vp94P14vZ8QQgjRim0HulkedzXTIl1HCCGEsIWWjCosFZ4QQghh\nT58DfZRSZUqpDGAO8JLPOi8B55qjC48CDmmtd8Y7o0IIIUQ4WtLi6qkUMQLWOcBZUclV9EW1e5ON\npGK5UrFMIOVKJqlYJpBytSpa63ql1OXAG4ATeFRrvUYpdbH5/EKM23amARuAKuD8OGczVY9dKpYr\nFcsEUq5kkoplAilXRMIanEkp9RQwAeNG293ATVrrvyulpgF301gp3haLTAohhBBCCCGEaL3CHlVY\nCCGEEEIIIYRIhJbc4yqEEEIIIYQQQsRcygeuSqmpSqn1SqkNSqnrE52fYJRS3ZRS7yml1iql1iil\nrjDTb1ZKbVdKrTT/pllec4NZtvVKqSmW9OFKqdXmc/cqpVQiymTJT7mZn5VKqS/MtLZKqbeUUt+Z\n/9tY1rd1uZRS/SzHY6VS6rBS6spkPFZKqUeVUnuUUl9b0qJ2bJRSmUqpp830z8w5oRNVrjuVUt8o\npVYppZ5XShWZ6aVKqWrLcVtox3IFKFPUzjmbHaunLWUqV0qtNNOT4liJ4JTUzVI3x6Y8Ujfb+Ps+\nQJmSul4OUi6pm2NRLq11yv5h3Hu7EegJZABfAQMTna8g+e0EHGcu5wPfAgOBm4Gr/aw/0CxTJlBm\nltVpPrcMGAUo4DXg1ASXrRwo9km7A7jeXL4euD3ZymU5z3YBPZLxWAHjgeOAr2NxbIBLgYXm8hzg\n6QSWazKQZi7fbilXqXU9n+3YplwByhS1c85Ox8rn+T8Dv0+mYyV/QY+31M02qcOQutm2ZQrwfZ/U\ndXOAMiV1vRykXFE75+xULp/n4143p3qL60hgg9Z6k9a6FlgEzEpwngLSWu/UWq8wlyuAdUCXIC+Z\nBSzSWtdorTdjjAw5UinVCSjQWi/VxtnwJHB6jLPfHLOAJ8zlJ2jMY7KVaxKwUWu9Jcg6ti2T1vpD\n4Ac/+Y3WsbFu61lgUjyuXPsrl9b6Ta11vflwKcY0XgHZrVwBjlUgSX2s3Mz3/ynwVLBt2LFcIiCp\nm+1Vh/mSutkGZUrFujkV62WQujnYNqJdrlQPXLsA31sebyN4ZWMbZnP5MOAzM+m/zW4Uj1q6hgQq\nXxdz2Tc9kTTwtlJquVJqvpnWQTfOGbgL6GAuJ1O5wLhKZP3gJvuxgugeG89rzMrpENAuNtmOyDyM\nK39uZWb3lg+UUuPMtGQpV7TOOTuVyW0csFtr/Z0lLZmPlZC62Tc9kaRuTq4ypXrdnEr1MkjdDFEu\nV6oHrklJKZUHPAdcqbU+DDyI0aVqKLATo2k+2YzVWg8FTgUuU0qNtz5pXoVJuiGulVIZwEzgGTMp\nFY6Vl2Q9NsEopW4E6oF/mUk7ge7mOXoV8G+lVEGi8hehlDvnfMzF+8dnMh8rkcSkbk4eUjcnnxSr\nlyEFzzkfCambUz1w3Q50szzuaqbZllIqHaNi/JfWejGA1nq31rpBa+0CHsHoZgWBy7cd764WCS+3\n1nq7+X8P8DxGGXabXQjcXQn2mKsnTbkwKvsVWuvdkBrHyhTNY+N5jVIqDSgE9scs5yEopc4DTgN+\nZlb8mF129pvLyzHuOelLEpQryuecLcrkZubhDOBpd1oyHyvhIXWzd3rCSN2cVGWCFK2bU61eBqmb\nidHxSvXA9XOgj1KqzLz6Ngd4KcF5Csjs1/13YJ3W+i+W9E6W1X4EuEf3egmYY47KVQb0AZaZ3UgO\nK6VGmds8F3gxLoXwQymVq5TKdy9j3Ij/NUb+f26u9nMa85gU5TJ5XXFK9mNlEc1jY93WbOBdd8UU\nb0qpqcC1wEytdZUlvUQp5TSXe2KUa1MylCvK55wtymRxCvCN1trTzSiZj5XwkLrZBt/3UjcDyVUm\nSMG6ORXrZZC6mVgdLx2HUakS+QdMwxgBcCNwY6LzEyKvYzG6fawCVpp/04B/AKvN9JeATpbX3GiW\nbT2WEe+AERgfko3AXwGVwHL1xBhB7Stgjfs4YPRjfwf4DngbaJtk5crFuDJUaElLumOFUbnvBOow\n7j24IJrHBsjC6K61AWNkuZ4JLNcGjPsp3J8v92h2PzbPzZXACmCGHcsVoExRO+fsdKzM9MeBi33W\nTYpjJX8hj7nUzYmvw6RutnGZAnzfJ3XdHKBMSV0vBymX1M0xKJf7hUIIIYQQQgghhC2leldhIYQQ\nQgghhBBJTgJXIYQQQgghhBC2JoGrEEIIIYQQQghbk8BVCCGEEEIIIYStSeAqhBBCCCGEEMLWJHAV\nQgghhBBCCGFrErgKIYQQQgghhLC1/wePp7Je+cgFyQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4ee90f0710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# TODO: Pick a network architecture here.\n",
    "#       The one below is just softmax regression.\n",
    "#\n",
    "\n",
    "model = FeedforwardNet(\n",
    "    [\n",
    "     DropoutLayer(0.2),\n",
    "     AffineLayer(784, 1600),\n",
    "     ReLULayer(),\n",
    "     DropoutLayer(),\n",
    "     AffineLayer(1600, 800),\n",
    "     ReLULayer(),\n",
    "     DropoutLayer(),\n",
    "     AffineLayer(800, 400),\n",
    "     ReLULayer(),\n",
    "     DropoutLayer(),\n",
    "     AffineLayer(400, 10),\n",
    "     SoftMaxLayer()\n",
    "    ])\n",
    "\n",
    "# Initialize parameters\n",
    "for p in model.parameters:\n",
    "    if p.name == 'W':\n",
    "        p.data.normal_(0, 0.01)\n",
    "        # p.data.uniform_(-0.1, 0.1) \n",
    "    elif p.name == 'b':\n",
    "        p.data.zero_()\n",
    "    else:\n",
    "        raise ValueError('Unknown parameter name \"%s\"' % p.name)\n",
    "\n",
    "# On lab computers you can set cuda=True !\n",
    "SGD(model, mnist_loaders, alpha=1e-1, cuda=False)\n",
    "\n",
    "print \"Test error rate: %.2f%%\" % compute_error_rate(model, mnist_loaders['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4: Data Augmentation [1p]\n",
    "\n",
    "Apply data augmentation methods (e.g. rotations, noise, crops) when training networks on MNIST, to significantly reduce test error rate for your network. You can use functions from the torchvision.transforms module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5: Batch Normalization [1p]\n",
    "\n",
    "*Covariate shift* is a phenomenon associated with training deep models. Simply put, weight changes in early layers cause major changes in distribution of inputs to later layers, making it difficult to train later layers.\n",
    "\n",
    "[Batch Normalization](https://arxiv.org/abs/1502.03167) addresses this problem by normalizing distributions of inputs to layers within mini-batches. It typically allows to train networks faster and/or with higher learning rates, lessens the importance\n",
    "of initialization and might eliminate the need for Dropout.\n",
    "\n",
    "Implement Batch Normalization and compare with regular training of MNIST models.\n",
    "\n",
    "Remember to use the batch statistics during model training and to use an average of training batch statistics during model evaluation. For details please consult the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Problem 6: Norm Constraints [1p bonus]\n",
    "\n",
    "Implement norm constraints, i.e. instead of weight decay, that tries to set \n",
    "all weights to small values, apply a limit on the total\n",
    "norm of connections incoming to a neuron. In our case, this\n",
    "corresponds to clipping the norm of *rows* of weight\n",
    "matrices. An easy way of implementing it is to make a gradient\n",
    "step, then look at the norm of rows and scale down those that are\n",
    "over the threshold (this technique is called \"projected gradient descent\").\n",
    "\n",
    "Please consult the Dropout paper (http://arxiv.org/pdf/1207.0580.pdf) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6: Polyak Averaging [1p bonus]\n",
    "\n",
    "Implement Polyak averaging. For each parameter $\\theta$\n",
    "keep a separate, exponentially decayed average of the past values\n",
    "$$\n",
    "\\bar{\\theta}_n = \\alpha_p\\bar{\\theta}_{n-1} + (1-\\alpha_p)\\theta_n.\n",
    "$$\n",
    "Use that average when evaluating the model on the test set.\n",
    "Validate the approach by training a model on the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 7: Convolutional Network [2p bonus]\n",
    "\n",
    "Use convolutional and max-pooling layers (`torch.nn.functional.conv2d`, `torch.nn.functional.max_pool2d`) and (without dropout) get a test error rate below 1.5%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
